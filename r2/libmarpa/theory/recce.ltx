% Copyright 2012 Jeffrey Kegler
% This document is licensed under
% a Creative Commons Attribution-NoDerivs 3.0 United States License.
\documentclass[12pt]{amsart}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{url}

\newcommand{\todo}[1]{\par{\large\bf Todo: #1}\par}
\newcommand{\mymathop}[1]{\mathop{\texttt{#1}}}

\newcommand{\dfn}[1]{{\bf #1}}
\newcommand{\sep}{\,\mid\,}
\newcommand{\mydot}{\raisebox{.05em}{$\,\bullet\,$}}
\newcommand{\cat}{\,.\,}
\newcommand{\size}[1]{\ensuremath{\left | {#1} \right |}}
\newcommand{\order}[1]{\ensuremath{{\mathcal O}(#1)}}
\newcommand{\Oc}{\order{1}}

% I use hyphens in variable names,
% so I need to ensure that subtraction is
% clearly distinguished by the typography
\newcommand{\subtract}{\,-\,}

\newcommand{\var}[1]{\ensuremath{\texttt{#1}}}

\newcommand{\cfg}{CFG}

\newcommand{\de}{\rightarrow}
\newcommand{\derivg}[1]{\mathrel{\mbox{$\:\Rightarrow\:$}}}
\newcommand{\derivrg}[2]{\mathrel{\mbox{$\:\stackrel{\!{#1}}%
        {\Rightarrow\!}\:$}}}

\newcommand{\set}[1]{{\left\lbrace #1 \right\rbrace} }
\newcommand{\mset}[2]{{\,\left\lbrace {#1} \,\middle\vert\, {#2} \right\rbrace} \,}
\newcommand{\ah}[1]{#1_{AH}}
\newcommand{\Vah}[1]{\ensuremath{\var{#1}_{AH}}}
\newcommand{\bool}[1]{\var{#1}_{BOOL}}
\newcommand{\Vbool}[1]{\ensuremath{\bool{#1}}}
\newcommand{\dr}[1]{#1_{DR}}
\newcommand{\Vdr}[1]{\ensuremath{\var{#1}_{DR}}}
\newcommand{\Vdrset}[1]{\ensuremath{\var{#1}_{\set{DR}}}}
\newcommand{\eim}[1]{#1_{EIM}}
\newcommand{\Veim}[1]{\ensuremath{\var{#1}_{EIM}}}
\newcommand{\Veimt}[1]{\ensuremath{\var{#1}_{EIMT}}}
\newcommand{\Veimx}[1]{\ensuremath{\var{#1}_{EIMX}}}
\newcommand{\Veimm}[1]{\ensuremath{\var{#1}_{EIMM}}}
\newcommand{\Veimset}[1]{\ensuremath{\var{#1}_{\set{EIM}}}}
\newcommand{\Veimtset}[1]{\ensuremath{\var{#1}_{\set{EIMT}}}}
\newcommand{\Veimxset}[1]{\ensuremath{\var{#1}_{\set{EIMX}}}}
\newcommand{\Veimmset}[1]{\ensuremath{\var{#1}_{\set{EIMM}}}}
\newcommand{\es}[1]{#1_{ES}}
\newcommand{\Ves}[1]{\ensuremath{\var{#1}_{ES}}}
\newcommand{\Vest}[1]{\ensuremath{\var{#1}_{EST}}}
\newcommand{\Vesi}[2]{\ensuremath{\var{#1}[#2]_{ES}}}
\newcommand{\VVes}[2]{\ensuremath{\var{#1}[\var{#2}]_{ES}}}
\newcommand{\Vlimt}[1]{\ensuremath{\var{#1}_{LIMT}}}
\newcommand{\Vlimm}[1]{\ensuremath{\var{#1}_{LIMM}}}
\newcommand{\Vlimx}[1]{\ensuremath{\var{#1}_{LIMX}}}
\newcommand{\loc}[1]{\var{#1}_{LOC}}
\newcommand{\Vloc}[1]{\ensuremath{\loc{#1}}}
\newcommand{\Vrule}[1]{\ensuremath{\var{#1}_{RULE}}}
\newcommand{\Vruleset}[1]{\ensuremath{\var{#1}_{\set{RULE}}}}
\newcommand{\Vsize}[1]{\ensuremath{\size{\var{#1}}}}
\newcommand{\Vstr}[1]{\ensuremath{\var{#1}_{STR}}}
\newcommand{\sym}[1]{#1_{SYM}}
\newcommand{\Vsym}[1]{\ensuremath{\var{#1}_{SYM}}}
\newcommand{\Vorig}[1]{\ensuremath{\var{#1}_{ORIG}}}
\newcommand{\symset}[1]{#1_{\lbrace SYM \rbrace} }
\newcommand{\Vsymset}[1]{\ensuremath{\var{#1}_{\set{SYM}}}}
\newcommand{\term}[1]{#1_{TERM}}
\newcommand{\token}[1]{#1_{TOK}}

\newcommand{\alg}[1]{\ensuremath{\textsc{#1}}}
\newcommand{\AH}{\ensuremath{\alg{AH}}}
\newcommand{\Earley}{\ensuremath{\alg{Earley}}}
\newcommand{\Leo}{\ensuremath{\alg{Leo}}}
\newcommand{\Marpa}{\ensuremath{\alg{Marpa}}}
\newcommand{\Emulator}{\ensuremath{\alg{Emulator}}}

\newcommand{\Vfa}{\var{fa}}
\newcommand{\Vg}{\var{g}}
\newcommand{\Vw}{\var{w}}
\newcommand{\VVw}[1]{\ensuremath{sym{\Vw[\var{#1}]}}}
\newcommand{\Vwi}[1]{\ensuremath{sym{\Vw[#1]}}}
\newcommand{\Vrules}{\var{rules}}
\newcommand{\GOTO}{\mymathop{GOTO}}
\newcommand\myL[1]{\operatorname{L}(#1)}
\newcommand\postdot[1]{\operatorname{postdot}(#1)}
\newcommand\tablei[1]{\es{\var{table}[#1]}}
\newcommand\Vtable[1]{\tablei{\var{#1}}}
\newcommand\Vtablei[2]{\var{table}(#1,\var{#2})}
\newcommand{\tablesizen}[1]{\size{\Vtablei{#1}{n}}}

% I want to use 'call' outside of pseudocode
\newcommand\call[2]{\textproc{#1}\ifthenelse{\equal{#2}{}}{}{(#2)}}%

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem*{definition}{Definition}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem{observation}[theorem]{Observation}


\begin{document}

\title{Marpa, a practical general parser: the recognizer}

\author{Jeffrey Kegler}
\thanks{
Copyright \copyright\ 2012 Jeffrey Kegler.
}
\thanks{
This document is licensed under
a Creative Commons Attribution-NoDerivs 3.0 United States License.
}

\date{\today}

\begin{abstract}
This document reports describes the recognizer portion
of the Marpa algorithm.
The Marpa algorithm is a practical, and fully implemented,
algorithm for the recognition,
parsing and evaluation of context-free grammars.
Marpa's recognizer is based
on Earley's algorithm,
and merges the improvements made
to Earley's
by Joop Leo
with those of Aycock and Horspool.
The Marpa parse engine has its own added feature:
full knowledge of the state of the parse is available
as tokens are being scanned.
Advantageous for error detection,
this knowledge also
allows
``Ruby Slippers'' parsing --
alteration of the input in reaction
to the parser's expectations.
\end{abstract}

\maketitle

\section{THIS IS AN EARLY DRAFT}

This a very early draft of this document,
and is largely unchecked.
The spirit and culture of the open source community
dictate that works of this kind and
at even this very early stage be made
available on-line in public repositories.
The reader should see the availability of this document
as compliance with that spirit and culture,
and not as in any way suggesting or
encouraging reliance on its contents.

\section{Introduction}

Despite the promise of general context-free parsing,
and the strong academic literature behind it,
it has never been incorporated into a tool
as widely available as yacc or
regular expressions.
The Marpa project was begun to end this neglect by
turning the best results from that literature
into a widely-available tool.
A stable version of this tool, Marpa::XS~\cite{Marpa-XS},
was uploaded to the CPAN Perl archive
on Solstice Day in 2011.

Sections
\ref{s:start-prelim} through \ref{s:end-prelim}
will outline notation and some of the
underlying concepts.
Section \ref{s:pseudocode} presents the pseudocode
for the algorithm Marpa.
Section
\ref{s:correctness}
contain a proof of correctness.
Section \ref{s:complexity} contains
the complexity results.
Finally,
section \ref{s:generalization} generalizes
the definitions of grammar and input.

\section{Preliminaries}
\label{s:prel}
\label{s:start-prelim}

We assume familiarity with the basic theory of parsing,
as well as Earley's algorithm.
This document will
use subscripts to indicate its commonly occurring types.
$X_T$ will be the variable $X$ of type $T$.
$/var{X-set}_{T}$ will be the variable $X$ of type set of $T$.
and $\sym{a}$ will be the variable \var{a} of type $SYM$,
where $SYM$ indicates a symbol.
$\symset{X-set}$ will be the set of symbols \var{X-set}.
Subscripts may be omitted when the type
is obvious from the context.
The notation for
constants will be the same as for variables.

Where \Vsymset{abced} is a set of symbols,
let $abced^\ast$ be the set of all strings
(type STR) formed
from those symbols.
Let $abced^+$ be the subset of $abced^\ast$ that
contains all of its elements that are not of zero length.

For the purposes of this document consider,
without loss of generality,
a grammar $g$,
and a set of symbols, $alphabet$.
Call the language of \var{g}, $\myL{g}$,
where $\myL{g} \in alphabet^\ast$
Divide $alphabet$ into two disjoint sets,
\Vsymset{lh} and \Vsymset{term}.

For the rewriting, designate a set of duples, \Vruleset{rules},
where $\forall \Vrule{r}, \Vrule{r} \in \Vruleset{rules}$,
\Vrule{r} takes the form $[\Vsym{lhs} \de \Vsymset{rhs}]$,
where $\Vsym{lhs} \in \var{term}$ and
$rhs \in alphabet^+$.
\Vsym{lhs} is referred to as the left hand side (LHS)
of \Vrule{r}.
\Vsymset{rhs} is referred to as the right hand side (RHS)
of \Vrule{r}.
This definition follows \cite{AH2002},
which departs from tradition by disallowing an empty RHS.

The grammar $g$ can be defined as the 4-tuple
$$(\Vsymset{alphabet}, \Vsymset{term}, rules, \Vsym{start})$$
where \Vsymset{term} is the
set of terminal symbols, $\Vsymset{term} \subset \Vsymset{alphabet}$.
The set of potential LHS symbols, \Vsymset{lh} is
$\var{alphabet} \setminus \var{term}$.
Without loss of generality,
it is assumed that \Vg
has a dedicated acceptance symbol
$\Vsym{accept}, \var{accept} \in \Vsymset{lh}$
and rule,
$\Vrule{accept} = [ \Vsym{accept} \de \Vsym{start} ] $
such that for every rule, $\Vrule{x} = [ \Vsym{lhs} \de \Vsymset{rhs} ]$
\begin{center}
\begin{tabular}{ll}
1.\hspace{.5em} &
$  \Vsym{accept} = \Vsym{lhs} \implies \Vrule{x} = \Vrule{accept} $ \\
2. &
$  \Vsym{accept} \notin \Vsymset{rhs} $ \\
\end{tabular}
\end{center}

Let the input to parse be \Vw, $\Vw \in \var{alphabet}^\ast$.
Locations in the input will be of type LOC.
Let \Vsize{w} be the length of the input, counted in symbols.
Let \VVw{i},
$\VVw{i} \in \Vsymset{term}$ be character \var{i}
of the input,
$0 \le \Vloc{i} < \Vsize{w}$.

We have already noted
that no rules of \Vg
have a zero-length RHS.
Further, no rule can be nullable
and all symbols must be either nulling or non-nullable --
no symbol can be a proper nullable.
These restrictions follow Aycock and Horspool~\cite{AH2002}.
The elimination of empty rules and proper nullables
is done by rewriting the grammar.
This can be done without loss of generality
and, in their paper~\cite{AH2002},
Aycock and Horspool
show how to do this
without effect on the time complexity
as a function of the input size.
Very importantly,
this rewrite is done in such a way that the semantics
of the original grammar can be efficiently reconstructed
at evaluation time.

Aycock and Horspool did allow a single empty start rule
to deal with null parses.
Marpa eliminates the need for empty rules in its grammars
by treating null parses and trivial grammars as special cases.
(Trivial grammars are those which recognize only the null string.)

In this document, \Earley{} will refer to the Earley's original
recognizer~\cite{Earley1970}.
\Leo{} will refer to Leo's revision of \Earley{}
as described in~\cite{Leo1991}.
\AH{} will refer to the Aycock and Horsool's revision
of \Earley{}
as described in~\cite{AH2002}.
Where $\alg{Recce}$ is a recognizer,
$\myL{\alg{Recce},g}$ will be the language accepted by $\alg{Recce}$
when parsing grammar $g$.

\section{Earley's algorithm}

Let $\Vrule{r} \in \Vrules$
be a rule,
and $\Vsize{r}$ the length of its RHS.
A dotted rule (type DR) is a duple, $[\Vrule{r}, \var{pos}]$,
where $0 < \var{pos} < \size{\Vrule{r}}$.
The position indicates the extent to which
the rule has been recognized,
and is represented with a raised dot,
so that if
$$[\Vsym{A} \de \Vsym{X} . \Vsym{Y} . \Vsym{Z}]$$
is a rule,
$$[\Vsym{A} \de \var{X} . \var{Y} \mydot \var{Z}]$$
is the rule where the dot is between \Vsym{Y}
and \Vsym{Z} ($\var{pos} = 2$).

The postdot symbol of a dotted rule is
often important.
$\postdot{\Vdr{x}} = \Vsym{last}$ if and only if
\begin{align*}
\Vdr{x} = & \Vsym{A} \de \Vstr{alpha} \mydot \Vsym{last} \Vstr{beta} \\
 & \qquad \text{where $\Vstr{beta} \derivrg{\ast}{\Vg} \epsilon$}
\end{align*}
Otherwise,
$\postdot{\Vdr{x}} = \Lambda$.

A traditional Earley item (EIMT) is the duple
\[
    [\Vdr{dotted-rule}, \Vloc{origin}]
\]
of dotted rule and origin.
(The origin is the location where recognition of the rule
started.
It is sometimes called the "parent".)
For convenience, the type ORIG will be a synonym
for LOC, indicating that the variable designates
the origin element of an Earley item.
The traditional Earley sets are sets in the strict
sense -- duplicate Earley items are not added.

An traditional Earley parser builds a table of Earley sets,
$\Vtable{i}, 0 \le \Vloc{i} < \size{\Vw}$.
Each set is the closure of the 
initialization, scanning, reduction and prediction
operations.
These four operations
are called in this document the {\bf basic
Earley operations}.

The sets are built in order from 0 to $\size{\Vw}$.
Recall that \Vrule{accept} was $[ \Vsym{accept} \de \Vsym{start} ]$
Define \Vdr{accept} to be $[ \Vsym{accept} \de \Vsym{start} \mydot ]$.
\Vw is accepted if and only if
$$[\Vdr{accept}, 0] \in \tablei{\Vsize{\Vw}}$$

We now describe the basic Earley operations.
In the following definitions,
\Vloc{j} is the current Earley set.

The {\bf initialization} basic Earley operation only takes
place in Earley set 0.
Earley set 0 is initialized to $\set{ [ \Vdr{initial}, 0 ] }$,
where
$$\Vdr{initial} = [\Vsym{accept} \de \mydot \Vsym{start} ]$$

A {\bf scanning} basic Earley operation takes place
in all Earley sets other than Earley set 0
for each pair $[\Veimt{pred}, \Vsym{token}]$,
\begin{center}
\begin{tabular}{rl}
i) & $\var{token} = \Vwi{j \subtract 1}$ \\
ii) & $\Veimt{pred} = [ \Vdr{before}, \Vloc{origin} ]$ \\
iii) & $[\Vsym{A} \de \Vstr{alpha} \cat \Vsym{token} \cat \Vstr{beta} ] \in \Vrules $ \\
iv) & $\Vdr{before} = [ \var{A} \de \var{alpha} \mydot \var{token} \cat \var{beta} ]$ \\
\end{tabular}
\end{center}
A scanning operation
adds \Veimt{result} to Earley set \Vloc{j},
where
\begin{center}
\begin{tabular}{rl}
i) & $\Veimt{result} = [ \Vdr{after}, \Vloc{origin} ] $\\
ii) & $\Vdr{after} = [ \var{A} \de \var{alpha} \cat \var{token} \mydot \var{beta} ]$
\end{tabular}
\end{center}
\Veimt{pred} is called the predecessor of the scanning operation
and \Veimt{result} is called its result.

An {\bf Earley reduction} basic operation is attempted
in every Earley set
for every pair $[\Veimt{pred}, \Vsym{lhs}]$,
where
\begin{center}
\begin{tabular}{rl}
ii) & $\Veimt{component} = [ \Vdr{complete}, \Vorig{component} ]$ \\
ii) & $\Veimt{component} \in \Vtable{j}$ \\
ii) & $\Vdr{complete} = [ \Vsym{lhs} \de \Vstr{rhs} \mydot ]$ \\
ii) & $\Veimt{pred} = [ \Vdr{before}, \Vorig{pred} ]$ \\
ii) & $\Veimt{pred} \in \tablei{\Vorig{component}}$ \\
ii) & $\Veimt{pred} = [ \Vdr{before}, \Vorig{x} ]$ \\
iii) & $[\Vsym{A} \de \Vstr{alpha} \cat \Vsym{lsh} \cat \Vstr{beta} ] \in \Vrules $ \\
iv) & $\Vdr{before} = [ \var{A} \de \var{alpha} \mydot \Vsym{lhs} \cat \var{beta} ]$ \\
\end{tabular}
\end{center}
A reduction operation
adds \Veimt{result} to Earley set \Vloc{j},
where
\begin{center}
\begin{tabular}{rl}
i) & $\Veimt{result} = [ \Vdr{after}, \Vloc{pred} ] $\\
ii) & $\Vdr{after} = [ \var{A} \de \var{alpha} \cat \var{lhs} \mydot \var{beta} ]$
\end{tabular}
\end{center}
\Veimt{component} is called the component of the reduction operation\footnote{
The term ``component'' comes from Irons \cite{Irons}.
}
\Veimt{pred} is called the predecessor of the reduction operation
and \Veimt{result} is called its result.

A {\bf prediction} basic Earley operation is attempted
in every Earley sets
for every \Veimt{pred}
\begin{center}
\begin{tabular}{rl}
ii) & $\Veimt{pred} = [ \Vdr{pred}, \Vorig{pred} ]$ \\
ii) & $\Veimt{pred} \in \Vtable{j}$ \\
ii) & $\Vdr{pred} = [ \Vsym{lhs} \de \Vstr{alpha} \mydot \Vsym{A} \cat \Vstr{beta} ]$ \\
\end{tabular}
\end{center}
The prediction operation
adds the members of \Veimtset{results}
to Earley set \Vloc{j},
where
\Veimtset{results} is of all EIMT's
$$[ \Vdr{prediction}, Vloc{j} ]$$
such that 
$$ \Vdr{prediction} = [ \Vsym{A} \de \mydot \Vstr{rhs} ] $$
for some 
$$[ \Vsym{A} \de \Vstr{rhs} ] \in \Vrules$$
\Veimt{pred} is called the predcessor of the prediction operation.
and the members of \Veimtset{results} are its results.

An EIMT is called the \dfn{cause} of an operation if it is
\begin{itemize}
\item The predecessor of a scanning, reduction or prediction operation.
\item The component of a reduction operation.
\end{itemize}
The cause of an operation is also the cause of all its result EIMT's.
An EIMT which is result of an scanning or reduction operation with a 
is called the \dfn{successor} of that predecessor.

\section{The Leo algorithm}

In \cite{Leo1991}, Joop Leo presented a method for
dealing with right recursion in \order{n} time.
Leo shows that,
With his modification, Earley's algorithm
is \order{n} for all LR-regular grammars.
(LR-regular is LR where lookahead
is infinite length, but restricted to
distinguishing between regular expressions.)

Summarizing Leo's method,
it consists of spotting potential right recursions
and memoizing them.
Leo restricts the memoization to situations where
the right recursion is unambiguous.
Potential right recursions are memoized by
Earley set, using what Leo called
``transitive items''.
In this document Leo's ``transitive items'' will be called Leo items.

In each Earley set, there is at most one Leo item per symbol.
The form of Leo items
from in Leo's original paper, is the triplet
$$[ \Vdr{top}, \Vsym{transition}, \Vorig{top} ]$$
where \Vsym{transition} is the transition symbol,
and $\Veimt{top} = [\Vdr{top}, \Vorig{top}]$
is the Earley to be added on reductions over
the \Vsym{transition}.
Leo items of this form will be called
``traditional'' Leo items (type LIMT).

Leo items memoize what would otherwise be sequences
of Earley items.
Leo items only memoize unambiguous (or
deterministic) sequences,
and so that top of the sequence can stand
in for the entire sequence --
the only role the other EIMT's in the sequence would
play in a parse is to derive the top EIMT.
We will call these memoized sequences, Leo sequences.

Each Leo sequence, if fully expanded would contain \order{n} items,
where \var{n} is the length of the sequence.
In \Earley, each such sequence would be expanded in every
Earley set which is the origin of an EIMT included in the
sequence, and the total number of EIMT's would be
\order{n^2}.

With Leo memoization, a single EIMT stands in for the sequence.
There are \Oc Leo items per Earley set,
so the cost of the sequence is \Oc per Earley set,
or \order{n} for the entire sequence.
If, at evaluation time,
it is desirable to expand the Leo sequence,
only those items actually involved in the parse
need to be expanded.
All the EIMT's of a potential right-recursion 
will be in one Earley set and the number of EIMT's
will be \order{n},
so that even including expansion of the Leo sequence
for evaluation, the time and space complexity of
the sequence remain \order{n}.

Implementing Leo memoization requires
adding another basic operation.
In addition to the basic operations of \Earley,
\Leo{} include Leo transition.
For any symbol in a reduction \Vsym{lhs},
a Leo transition, if one exists, replaces
the reduction operation.

In every Earley set after the first,
for every symbol \Vsym{lhs}
where \Earley would look for pairs
$[\Veimt{pred}, \Vsym{lhs}]$
such that
\[
    \Veimt{pred} \in \Vtablei{\Earley}{i} \\
    \text{and} \postdot{\var{pred}} = \Vsym{lhs}
\]
in order
to perform reduction operations,
Leo first attempts a Leo transition.
If there is a Leo transition for \Vsym{lhs} from
Earley set \var{i},
reduction on \Vsym{lhs} from Earley set \var{i}
is not attempted.

A Leo transition from \Vloc{i} over \Vsym{lhs} is possible
if and only if there exists
\Vlimt{pred}, \Vdr{top}, \Vorig{top}
such that
\begin{center}
\begin{tabular}{rl}
                 & $\Vlimt{pred} \in \Vtablei{\Earley}{i}$ \\
\text{and} & $\Vlimt{pred} = [ \Vdr{top}, \Vsym{lhs}, \Vorig{top} ]$
\end{tabular}
\end{center}
If there is a Leo transition,
$[ \Vdr{top}, \Vorig{top} ]$ is added to
\Vtablei{\Leo}{j}.
Otherwise, reduction is tried.

\section{The Aycock-Horspool finite automaton}
\label{s:AHFA}
\label{s:end-prelim}

In this document a
``split LR(0) $\epsilon$-DFA''
a described by Aycock and Horspool~\cite{AH2002},
will be called an Aycock-Horspool Finite Automaton,
or AHFA.
A full description of how to derive an AHFA in theory
can be found in \cite{AH2002},
and examples of how to derive it in practice
can be found in the code for Marpa\cite{Marpa-R2,Marpa-XS}.
Here I will summarize those ideas behind AHFA's
that are central to Marpa.

Aycock and Horspool based their AHFA's
on a few observations.
\begin{itemize}
\item
In practice, Earley items with the same dotted rule
often appear in groups in the Earley sets,
where the entire shares the same origin.
\item
There was already in the literature a method
for associating groups of dotted rules that often appear together
when parsing.
This method was the LR(0) DFA used in the much-studied
LALR and LR parsers.
\item
The LR(0) items that are the components of LR(0)
states are, exactly, dotted rules.
\item
By taking into account symbols which derive the
null string, the LR(0) DFA could be turned into an
LR(0) $\epsilon$-DFA, which would be even more effective
at grouping dotted rules which occur together.
\end{itemize}

AHFA states are sets of dotted rules.
Aycock and Horspool realized that by changing Earley items
to track AHFA states, instead of individual dotted rules,
the size of Earley sets could be reduced,
and Earley's algorithm made faster in practice.
In short, then, AHFA states are a shorthand that Earley items
can use for groups of dotted rules that occur together frequently.
The original Earley items could be represented as $(\Vdr{r}, origin)$
duples, where \Vdr{r} is a dotted rule.
Aycock and Horspool modified their Earley items to be $(\ah{L}, origin)$
duples, where $\ah{L}$ is an AHFA state.

\begin{definition}
A dotted rule is {\bf Marpa-valid} if and only if
it does not have a nulling postdot symbol.
\end{definition}

AHFA states are of two kinds:
{\bf Discovered AHFA states}
contain the predicted start rule and
discovered rules.
{\bf Predicted AHFA states}
contain predicted rules
other than the start rule.
(In \cite{AH2002} discovered states are called the ``kernel states'',
and predicted states are called ``non-kernel states''.)

It is important to note that
an AHFA is not a partition of the dotted
rules --
a single dotted rule can occur
in more than one AHFA state.
This does not happen frequently,
but it does happens often enough,
even in practical grammars,
that the Marpa implementation has to provide for it.

What does not seem to happen in practical grammars
the size of \AH Earley set to grow larger
than that of one of Earley's original sets.
That is, it seems the AHFA is always a win,
at least for practical grammars.

The following observations
will be useful.
They are shown in \cite{AH2002}.

\begin{observation}
AHFA's are monotone.
The result of an AHFA transition
contains the 
Any dotted
That is, if
\[
    \GOTO(\Vah{from}, \Vsym{t}) = \Vah{to} \land \Vdr{from} \in \Vah{from}
\]
then
\[
    \GOTO(\Vdr{from}, \Vsym{t}) = \Vdr{to} \land \Vdr{to} \in \Vah{to}
\]
\end{observation}

\begin{observation}
AHFA's predict correctly and completely.
\end{observation}

For time and space complexity purposes,
use of the AHFA states in EIM's can
be proved to be break-even.
The next theorem is useful for that purpose.

\begin{theorem}\label{t:leo-singleton}
If the AHFA state of
a Marpa Earley item (EIMM) is the result of a
Leo reduction,
then its AHFA state contains only one dotted rule.
\end{theorem}

\begin{proof}
Since the Earley item is the result of a Leo reduction,
we know that its AHFA state contains a completed rule.
Call that completed rule, \Vdr{complete}.
Let \Vrule{c} be the rule of \Vdr{complete},
and \var{cp} its dot position.
$\var{cp} > 0$ because completions are never
predictions.

Suppose, for a reduction to absurdity,
that the AHFA state contains another dotted rule,
\Vdr{other},
$\Vdr{complete} \neq \Vdr{other}$.
Let \Vrule{o} be the rule of \Vdr{other},
and \var{op} its dot position.
AHFA construction never places a prediction in the same
AHFA state as a completion, so
\Vdr{other} is not a prediction.
Therefore, $\var{op} > 0$.

To create a contradiction, we first prove that
$\Vrule{c} \neq \Vrule{o}$,
then that
$\Vrule{c} = \Vrule{o}$.
By the construction of an AHFA
state, both dotted rules resulted from the same series
of transitions.
But the same series of transitions over the
same rule would result in the same dot position,
$\var{cp} = \var{op}$
so that if $\Vrule{c} = \Vrule{o}$,
$\Vdr{complete} = \Vdr{other}$,
which is contrary to the assumption for the reduction.
Therefore, under the assumption for the reduction,
$\Vrule{c} \neq \Vrule{o}$.

Next we show that, under the assumption for the reduction,
that $\Vrule{c} = \Vrule{o}$
follows from \Leo's uniqueness requirement.
Since both dotted rules are in the same EIMM
and neither is a prediction,
both must result from transitions,
and their transitions must have been from the same Earley set.
Since they are in the same AHFA state,
by the AHFA construction,
that transition must have been
over the same transition symbol.
\Leo requires that, for each predecessor Earley set and transition symbol,
that a Leo transition be from a single rule.
The assumption for the reduction is that \Vdr{complete}
and \Vdr{other} are in the same Leo reduction state,
and in order for them to have obeyed the Leo uniqueness
requirement,
we have $\Vrule{c} = \Vrule{o}$.

We now have both
$\Vrule{c} = \Vrule{o}$
and
$\Vrule{c} \neq \Vrule{o}$,
completing the reduction to absurdity.
When \Vdr{complete} is in a Leo reduction EIMM,
it must be the only dotted rule in that EIMM.
\end{proof}

\section{The Marpa Recognizer}
\label{s:recce}
\label{s:pseudocode}

As mentioned, it is assumed that the reader
is familar with \Earley{}
and with the description of \AH{}
in \cite{AH2002}.
The comments to the \Marpa{} pseudocode
will focus on its differences
from those two algorithms.

With the pseudocode are observations
about its space
and time complexity.
In what follows,
we will show that all time and space resources
can be charged to attempts to add Earley items,
in a way that each attempt to add an Earley item
takes amortized \Oc{} resource.
Below, when we present the complexity proofs,
we will characterize
the orders of magnitude of actual Earley items
and of attempts to add Earley items,
as well as their relationship to each other.

To achieve the
amortized \Oc{} time and space
per Earley item, we will charge for those
resources in
three ways.

\begin{itemize}
\item We can also charge \Oc{} time and space to the parse itself.
We handle the failure cases in this way.
\item We will charge resources to the Earley set.
As long as the time or space is \Oc,
the time for the Earley set can be
re-charged to an arbitrary member of the Earley set.
If the Earley set is empty, the parse will fail at this Earley set,
and the time can be charged to the parse itself.
\item We will charge resources to attempts to add Earley items.
\end{itemize}

When discussing each procedure, we will state whether
our analysis of time and space usage is inclusive, exclusive
or caller-included.
The exclusive time or space of a procedure is that
which it uses directly,
ignoring resource usage by called procedures.
Inclusive time or space of a procedure includes
resource usage by called procedures.
Caller-included time and space is that which was already
been allocated as an inclusive resource by the procedure's
caller.

\begin{algorithm}[h]
\caption{Marpa Top-level}
\begin{algorithmic}[1]
\Procedure{Main}{}
\State \Call{Initial}{}
\For{ $\var{i}, 0 \le \var{i} \le \Vsize{w}$ }
\State \Comment At this point, $\tablei{\var{x}}$ is complete, for $0 \le \var{x} < \var{i}$
\State \Call{Scan pass}{$\var{i}, \var{w}[\var{i} \subtract 1]$}
\State reject if $\size{\tablei{\var{i}}} = 0$
\State \Call{Reduction pass}{\var{i}}
\EndFor
\If{$[\Vah{accept}, 0] \in \tablei{\Vsize{w}}$}
\State accept \var{w}
\Else
\State reject \var{w}
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Top-level code}

Exclusive time and space for the loop over the Earley sets
is charged to the Earley sets,
and overhead is charged to the parse.
All these resource charges are obviously \Oc.

\subsection{Ruby Slippers parsing}
This top-level code represents a significant change
from \AH{}.
\call{Scan pass}{} and \call{Reduction pass}{}
are separated.
As a result,
when scanning of tokens which start at location \Vloc{i} begins,
the Earley sets for all locations prior to \Vloc{i} are complete.
This means that the scanning operation has available, in
the Earley sets,
full information about the current state of the parse,
including which tokens are acceptable during the scanning phase.

This allows highly
accurate and flexible error detection,
but that is just the beginning.
It can be known immediately if a token
will be rejected, and the input can changed in
the light of the parser's expectations.
This means that error detection,
in many parsers an desperate last resort,
is recoverable and inexpensive,
allowing its use as a parsing technique.
Because this can be described as making the parser's
``wishes'' come true, I have called this ``Ruby Sippers
Parsing''.

One use of the Ruby Slippers technique is to
parse with a very clean,
but oversimplified grammar,
programming the lexical analyzer to make up for the grammar's
short-comings on the fly.
The author has implemented an HTML parser\cite{Marpa-HTML},
based on an grammar that assumes that all start
and end tags are present.
Such an HTML grammar is too simple even to describe fully
standard-conformant HTML, but by programming the lexical
analyzer to supply start and end tags as requested by the parser,
the application can parse very liberal HTML
and will produce a parse even for highly defective HTML.

\begin{algorithm}[h]
\caption{Initialization}
\begin{algorithmic}[1]
\Procedure{Initial}{}
\State \Call{Add EIM Pair}{$0, \ah{start}, 0$}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Initialization}
Inclusive time and space is \Oc{} and
and can be charged to the parse.

\begin{algorithm}[h]
\caption{Marpa Scan pass}\label{a:scan}
\begin{algorithmic}[1]
\Procedure{Scan pass}{$\Vloc{i},\Vsym{a}$}
\For{each $\Veimm{predecessor} \in \var{transitions}(\tablei{\var{i} \subtract 1},\Vsym{a})$}
\State $[\Vah{from}, \Vloc{origin}] \gets \Veimm{predecessor}$
\State $\Vah{to} \gets \GOTO(\Vah{from}, \Vsym{a})$
\State \Call{Add EIM Pair}{$\Vloc{i}, \Vah{to}, \Vloc{origin}$}
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Scan pass}

\var{transitions} is a set of tables, one per Earley set,
which is indexed by symbol.
Symbol indexing is \Oc, since the number of symbols
is a constant, but
for the operation $\var{transitions}(\Vloc{l}, \Vsym{s})$
to be constant, there must be a link directly to the Earley
set.
In the case of scanning,
the lookup is always in the previous Earley set,
and it is safe to expect this can be found
in \Oc{} time.
Inclusive time and space can be charged to the
Earley item attempts.

\begin{algorithm}[h]
\caption{Reduction pass}
\begin{algorithmic}[1]
\Procedure{Reduction pass}{\Vloc{i}}
\For{each Earley item $\Veimm{work} \in \tablei{i}$}
\State $[\Vah{work}, \Vloc{origin}] \gets \Veimm{work}$
\For{each lhs of a completed rule, $\Vsym{lhs}$}
\State \Call{Reduce one LHS}{\Vloc{i}, \Vloc{origin}, \Vsym{lhs}}
\EndFor
\State \Call{Memoize transitions}{\Vloc{i}}
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Reduction pass}

Exclusive time is clearly \Oc{}
and may be charged to each \Veimm{work}.
Overhead may be charged to the Earley set.

\begin{algorithm}[h]
\caption{Memoize transitiions}
\begin{algorithmic}[1]
\Procedure{Memoize transitions}{\Vloc{i}}
\For{every \Vsym{postdot}, a postdot symbol of $\tablei{i}$}
\If{postdot transition is unique by rule}
\State Set $\var{transitions}(\Ves{i},\Vsym{postdot})$ to
\State \hspace\algorithmicindent to an LIMX
\Else
\State Set $\var{transitions}(\Ves{i},\Vsym{postdot})$ to
\State \hspace\algorithmicindent to the set of EIMX's which have
\State \hspace\algorithmicindent with \Vsym{postdot} as their postdot symbol
\EndIf
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Memoize transitions}

The \var{transitions} table for Earley set \Vloc{i}
is built once all EIMMs have been
added to Earley set \Vloc{i}.
This can be done in
a single pass over Earley set \Vloc{i},
in \Oc{} time per EIMM.
The time and space required are charged to the
Earley items being examined.
Any loop overhead may be charged to the Earley set.

Of special note is that in
setting up the \var{transitions} table for the current Earley set,
the Leo items must be created.
For each Earley set,
the number of Leo items is at most
\Vsize{alphabet}.
Since the maximum number of Leo items is a constant
their space may be charged to the Earley set.

\begin{algorithm}[h]
\caption{Reduce one LHS symbol}
\begin{algorithmic}[1]
\Procedure{Reduce one LHS}{\Vloc{i}, \Vloc{origin}, \Vsym{lhs}}
\State \Comment $pim$ is a ``postdot item'', either a LIMX or an EIMM
\For{each $pim \in \var{transitions}(\Vloc{origin},\Vsym{lhs})$}
\If{\var{pim} is a LIMX, \Vlimx{pim}}
\State $\Vlimx{pred} \gets \var{pim}$
\State Perform \Call{Leo reduction operation}{\Vloc{i}, \Vlimx{pred}}
\Else
\State $\Veimx{pred} \gets \var{pim}$
\State Perform \Call{Earley reduction operation}{\Vloc{i}, \Veimx{pred}, \Vsym{lhs}}
\EndIf
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Reduce one LHS}

Inclusive time is charged to the EIMM attempt.
We show that
$\var{transitions}(\Ves{origin},\Vsym{lhs})$
can be done \Oc{} time
by noting
that the number of symbols is a constant
and assuming that \Veimm{x} has a link back
to its origin Earley set.
(As implemented, Marpa's
Earley items have such links.)

\begin{algorithm}[h]
\caption{Earley reduction operation}
\begin{algorithmic}[1]
\Procedure{Earley reduction operation}{\Vloc{i}, \Veimm{from}, \Vsym{trans}}
\State $[\Vah{from}, \Vloc{origin}] \gets \Veimm{from}$
\State $\ah{to} \gets \GOTO(\Vah{from}, \Vsym{trans})$
\State \Call{Add EIM Pair}{\Vloc{i}, \Vah{to}, \Vloc{origin}}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Earley Reduction operation}

Exclusive time and space is clearly \Oc.
Inclusive time and space is charged to the
calling procedure.

\begin{algorithm}[h]
\caption{Leo reduction operation}
\begin{algorithmic}[1]
\Procedure{Leo reduction operation}{\Vloc{i}, \Vlimm{from}}
\State $[\Vah{from}, \Vsym{trans}, \Vloc{origin}] \gets \Vlimm{from}$
\State $\ah{to} \gets \GOTO(\Vah{from}, \Vsym{trans})$
\State \Call{Add EIM Pair}{\Vloc{i}, \Vah{to}, \Vloc{origin}}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Leo Reduction operation}

Exclusive time and space is clearly \Oc.
Inclusive time and space is charged to the
calling procedure.

\begin{algorithm}[h]
\caption{Add EIM Pair}\label{a:pair}
\begin{algorithmic}[1]
\Procedure{Add EIM Pair}{$\Vloc{i},\Vah{reduced}, \Vloc{origin}$}
\State $\Veim{reduced} \gets [\Vah{reduced}, \Vloc{origin}]$
\State $\Vah{predicted} \gets \GOTO(\Vah{reduced}, \epsilon)$
\If{$\Veim{reduced}$ is new}
\State Add $\Veim{reduced}$ to $\tablei{i}$
\EndIf
\If{$\Vah{predicted} \neq \Lambda$}
\State $\Veim{predicted} \gets [\Vah{predicted}, \Vloc{i}]$
\If{$Veim{predicted}$ is new}
\State Add $\Veim{predicted}$ to $\tablei{i}$
\EndIf
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Adding a pair of Earley items}

This operation adds a reduced EIMM
item and, if it exists, the EIMM for
its null-transition.
Inclusive time and space is charged to the
calling procedure.
Trivially, the space is \Oc per call.

We show that time is also \Oc{}
by singling out two non-trivial cases:
checking that an Earley item is new,
and adding it to the Earley set.
It becomes clear
that an Earley item can be added to the current
set in \Oc{} time
if Earley set is seen as a linked
list, to the head of which the new Earley item is added.

Earley items are only added if they are new,
and showing that this check can be performed in \Oc time
per EIMM (and therefore per call)
is more complicated.
A data structure that allows this
to be checked in \Oc{} time is very
briefly outlined in both
\cite[p. 97]{Earley1970} and
\cite[Vol. 1, pages 326-327]{AU1972}.
The data structures used are not named in either source,
and in this document they will be called ``per-Earley set lists'',
or PSL's.
PSL's are the subject of the next section.

\subsection{Per-set lists}

A per-set list (PSL) is kept in each Earley set.
When building a new Earley set, \Ves{j},
the PSL for every previous Earley set, \Ves{i},
keeps a list of those Earley items in \Ves{j} which have \Ves{i}
as their origin.
Each PSL needs to only keep track of a maximum
number of possible Earley items.
For traditional Earley items that maximum is the number
of dotted rules.
For Marpa, that maximum is the number of AHFA states.
In both cases,
the maximum is a constant which depends on the grammar.

Clearing and rebuilding the PSL's every time
a new Earley set would take more than \Oc{},
but can be avoided.
The per-AHFA state entries can be ``stamped'',
with a link back to the Earley set
which was current when that PSL
entry was last added or updated.

Consider the case where Marpa is building \Ves{j}
an Earley item \Veimm{x} is new,
where $\Veimm{x} = [ \Vah{x}, \Vloc{origin} ]$.
To do this,
Marpa checks the PSL's for $\tablei{origin}$.
Let the Earley set ``stamp'' for that PSL entry be \Ves{p}.
If the entry has never been used, $\Ves{p} = \Lambda$
If $\Ves{p} \ne \Lambda \land \Ves{p} = \Ves{j}$,
then \Veimm{x} is not new,
and will not be added to the Earley set.

If $\Ves{p} = \Lambda \lor \Ves{p} \ne \Ves{j}$, then \Veimm{x} is new.
\Veimm{x} is added to the Earley set,
and the PSL's ``stamp'' is updated to \Ves{j}.
As implemented,
Marpa uses PSL's for this purpose and several others.

\section{Nulling symbols}
\label{s:nulling}

We have already noted that Marpa grammars
do not contain empty rules or
properly nullable symbols,
and shown that this is without loss
of generality.
This corresponds to the Marpa implementation.
For the correctness and comlexity proof in this section,
we assume a further rewrite not in the implementation,
this time to eliminate nulling symbols.

This additional rewrite is also
without loss of generality, as can be seen
if we assume that a history
of the rewrite is kept,
and that the rewrite is reversed
after the parse.
Clearly, whether an input \Vw{} is acceptable
or will not depend on how often
a grammar \Vg{} interpolates nulling symbols into
its input stream.

A question for future research is whether rewriting
the grammar to eliminate nulling symbols during parsing
would in fact be a better implementation.
Efficiency gains, if they existed,
would probably not be impressive,
but the rewrite of the Earley items after
the parse would allow the nulling symbols
to be added based on full knowledge of the
result of the parse.

This might allow some new parsing
techniques.
Optionally introducing nulling symbols could "tag"
other symbols, and might be a natural way to add
a degree of context-sensitivity to \Marpa.

\section{Marpa recognizer correctness}
\label{s:correctness}

\begin{definition}
A Marpa Earley set \EVtable{\Marpa,i}
is \definition{correct} if and only if for all
\[
[\Vah{x}, \Vorig{x}] \in \EVtable{\Marpa}{i}
\]
there exists
\[
[\Vdr{y}, \Vorig{x}] in \EVtable{\Leo}{i}
\]
such that
\Vdr{y} \in \Vah{x}
\end{definition}

\begin{definition}
A Marpa Earley set \EVtable{\Marpa,i}
is \definition{complete} if and only if for all
\[
[\Vdr{x}, \Vorig{x}] in \EVtable{\Leo}{i}
\]
there exists
\[
[\Vah{x}, \Vorig{x}] \in \EVtable{\Marpa}{i}
\]
such that
\Vdr{y} \in \Vah{x}
\end{definition}

\begin{definition}
We say that
a Marpa Earley set \EVtable{\Marpa}{i}
and a Leo Earley set \EVtable{\Leo}{i}
are \definition{congruent}
($EVtable{\Leo}{i} \cong EVtable{\Marpa}{i}$)
if and only the Marpa Earley set is complete
and correct.
\end{definition}

\begin{theorem}\label{congruence}
\begin{equation*}
\forall \Vloc{i}, 0 \le \var{i} < \Vsize{w} \implies \Vest{i} \cong \Ves{i}
\end{equation*}
\end{theorem}

\begin{proof}
The proof is by a induction on the Earley sets.
We leave it as an exercise to show, as the
basis of the induction, that
\[
\EEtable{\Earley}{0} \conq \EEtable{\Marpa}{i}
\]
We show correctness by 
an inner induction on the Marpa operations.
We have the basis
trivially, since an empty Marpa Earley set is
correct.

We show the step of the inner induction by cases.
For Marpa's scanning operation, we know
we have a correct predecessor EIM by the induction hypothesis,
and by definitions in the preliminaries,
we know that the tokens will be the same for both Earley and Marpa.
\todo{Talk about AHFA correctness}
\end{proof}

\section{Marpa recognizer complexity}
\label{s:complexity}

\subsection{Nulling symbols}
For the complexity proofs,
we consider only Marpa grammars without nulling
symbols.
When we examined correctness,
we showed that this rewrite
is without loss of generality
\ref{s:nulling}.
For complexity we must also show that
the rewrite and its reversal can be done
in amortized \Oc{} time and space
per Earley item.

One way to show the required bound is
to allocate
the time and space involved in rewriting Earley
items with nulling symbols directly to
the Earley item to which they are rewritten
and from which they can be restored.
Since
the number of occurrences of nulling symbols in any rule
or in any AHFA state is a constant depending on the grammar,
the time and space involved are also constant,
and our bound follows.

\subsection{The order of Marpa's Earley items}

\begin{theorem}\label{t:eim-order}
Let \Vg{} be a grammar as defined in this
document,
and let \Vfa{} be the AHFA for this grammar.
Let \tablesizen{\Marpa} be the number of Earley items in
a \Marpa{} parse,
and \tablesizen{\Leo} the number of Earley items in a \Leo
parse.
$$ \order{\tablesizen{\Marpa}} = \order{\tablesizen{\Leo}} $$
\end{theorem}

\begin{proof}
Let \VVes{L}{i} be
an Earley set of \Leo.
Let \VVes{El}{i} be an
an Earley set of \Emulator{}
in Leo mode.
By Theorem \ref{t:leo-congruence},
we know that that for every EIML
$$[\Vdr{l}, \Vloc{origin}] \in \Vesi{L}{i}$$
there is at least one EIMX,
$$[\Vbool{b}, \Vah{el}, \Vdr{l}, \Vloc{origin}] \in \Vesi{El}{i}
$$
We also know that for every EIMX,
$$ [\Vbool{b}, \Vah{el}, \Vdr{l}, \Vloc{origin}] \in \Vesi{L}{i}
$$
that
$$[\Vdr{l}, \Vloc{origin}] \in \Vesi{L}{i}$$

From this we see that there is a total function
from
$\size{\Vesi{L}{i}}$ to a partition
of $\size{\Vesi{El}{i}}$,
such that $\Veimt{x} \mapsto \Veimxset{y}$,
where
\begin{align*}
&& \Veimt{x} = & [ \Vdr{x}, \Vloc{x} ] \\
& \land & \Veimx{y} = &
\set{ [ \Vbool{z}, \Vah{z}, \Vdr{x}, Vloc{x} \sep \Vdr{x} \in \Vah{z} ] }
\end{align*}

By the definition of \Emulator,
EIMX's that differ only in \Vbool{z} will not appear
in the same Earley set,
and the number of AHFA states, \Vsize{fa}, is finite.
The theorem follows.
Therefore
$$ \size{\VVes{El}{i}} \le \size{fa} \times \size{\VVes{L}{i}} $$

From Theorem {t:emulator-equivalence},
we have
$$ \size{\VVes{Em}{i}} = \size{\VVes{El}{i}} $$

And by the congruence of \Emulator in Marpa mode
with Marpa,
we know there is a bijection from a partition of the
Earley items in \VVes{Em}[i] to \VVes{M}[i],
so that
$$ \size{\VVes{M}{i}} \le \size{\VVes{Em}{i}} $$

Collecting the last three relations, we have
$$ \size{\VVes{M}{i}} \le \size{fa} \times \size{\VVes{L}{i}} $$

And summing over $\var{i}, 0 \le \var{i} \le \var{n}$
where $\var{n} = \Vsize{w}$,
we see that
$$ \order{\tablesizen{\Marpa}} = \order{\tablesizen{\Leo}} $$
\end{proof}

\subsection{Amortized complexity of Marpa's Earley items}

\begin{theorem}\label{t:O1-per-eim}
All time in \Marpa{} can be allocated
to the Earley items,
and in such a way that processing each Earley item
requires \Oc{} time and space.
\end{theorem}

\begin{proof}
For some the operations of \Earley,
it is known that the amortized time
per Earley item
is amortized $\order(1)$\cite[Vol. 1, pages 326-327]{AU1972}.
Inspection of the algorithm for \Marpa{} will show
that all space
and time
for any operations new to \Marpa
can be assigned to the Earley items in
an obvious way,
and trivially shown to be \Oc.
\end{proof}

\subsection{Marpa complexity}

All of the complexity results for \Leo are
based on the order of magnitude of the
sum of the cardinalities of its Earley sets.
From
Theorems \ref{t:eim-order} and \ref{t:O1-per-eim},
we see that the
order of magnitude of
the time
and space complexity of \Marpa{} is never worse than
that of \Leo.
Because
Theorem 4.3 of \cite[p. 172]{Leo1991}
shows that the
order of magnitude of
the time
and space complexity of \Leo is never worse than
that of \Earley,
we also see that the
order of magnitude of
the time
and space complexity of \Marpa{} is also
never worse than that of \Earley.

We now state some of the more important
specific results.
In the following theorems,
\var{n} is the length of the input, \Vsize{w}.

\begin{theorem}
For every LR-regular grammar,
\Marpa{} runs in $\order{n}$ time and space.
\end{theorem}

\begin{proof}
By Theorem 4.6 in \cite[p. 173]{Leo1991},
and Theorems \ref{t:eim-order} and \ref{t:O1-per-eim}.
\end{proof}

\begin{theorem}
For every unambiguous grammar,
\Marpa{} runs in $\order{n^2}$ time.
\end{theorem}

\begin{proof}
The time complexity of \Earley for unambiguous grammars
is $\order(n^2)$
(Theorem 4.10 in \cite[Vol 1, p. 327]{AU1972};
\cite{Earley1970}).
The results follows by using
Theorem 4.3 of \cite[p. 172]{Leo1991},
and Theorems \ref{t:eim-order} and \ref{t:O1-per-eim}
\end{proof}

\begin{theorem}
For any context-free grammar,
\Marpa{} runs in $\order{n^3}$ time
and $\order{n^2}$ space.
\end{theorem}

\begin{proof}
The time complexity of \Earley is $\order(n^3)$
and its space complexity is $\order(n^2)$\cite{Earley1970}.
The results follows by using
Theorem 4.3 of \cite[p. 172]{Leo1991},
and Theorems \ref{t:eim-order} and \ref{t:O1-per-eim}.
\end{proof}

\section{Generalizing the grammar}
\label{s:generalization}

As implemented,
Marpa generalizes the idea of grammars
and input streams beyond that so far described
for \Vg{} and \Vw.
Because the differences were
minor from a theoretical point of view,
and their discussion has been deferred to avoid
cluttering the proofs.
This section redefines \Vg{} and \Vw,
to incorporate the
deferred generalizations.

First, Marpa's grammars are in effect 3-tuples:
$$(\Vsymset{alphabet}, rules, \Vsym{start})$$
\Vsymset{term} is omitted, because
Marpa allows a symbol to be both a terminal
and a LHS.
This expansion of the grammar definition
is made without loss of generalization,
or effect on the results.
\footnote{
Marpa has options which
cause the traditional restrictions to
be enforced,
in part or in whole.
For error detection and efficiency,
users may well prefer this.
}

Second, Marpa's input model is a generalization of
the traditional input stream model
used so far.
Marpa's input is a set of tokens,
$toks$,
whose elements are triples of symbol,
start location and end location:
$$(\Vsym{t}, \Vloc{start}, \var{length})$$
such that
$$\var{length} \ge 1 \wedge \Vloc{start} \ge 0$$
The size of the input, \Vsize{toks} is the maximum over
\var{toks} of $\Vloc{start}+\var{length}$.
Tokens may overlap,
but gaps are not allowed:
\begin{align*}
    & \forall \Vloc{i} (\exists \token{t} \sep \token{t} \in \var{toks} \\
\wedge & t = (\Vsym{t}, \Vloc{start}, \var{length}) \\
\wedge & \Vloc{start} \le \Vloc{i} \le \Vloc{start}+\var{length}
\end{align*}

The traditional input stream is the special case of
a Marpa input stream where
\begin{gather*}
\forall \token{tok} \sep \token{tok} \in \var{toks} \\
\implies \token{tok} = [\Vsym{s}, \Vloc{start}, 1]
\end{gather*}
and for all $\token{tok1}, \token{tok2}$ in \var{toks}
\begin{center}
\begin{tabular}{rl}
(i) & $\token{tok1} = [\sym{s1}, \loc{start1}, 1]$ \\
(ii) & $\token{tok2} = [\sym{s2}, \loc{start2}, 1]$ \\
(iii) & $\loc{start1} = \loc{start2} \implies \token{tok1} = \token{tok2}$ \\
\end{tabular}
\end{center}

The correctness results hold for Marpa input streams,
but to preserve the time complexity bounds,
certain restriction must be imposed on them.
Call an Earley set, $\es{table[j]}$
a ``lookahead set'' at $\loc{i}$
if $j>i \wedge$
and there exists some token $\token{tok} = (\sym{s}, \loc{start}, length)$
such that $\loc{start} \le i \wedge \loc{start}+length = \loc{j}$.
If, at every location,
the number of tokens which start there,
and the number of lookahead sets in play at that location
is less than a finite constant,
then Earley items can be added in amortized constant time
and the complexity results for
\Marpa{} stand.

\bibliographystyle{plain}

\begin{thebibliography}{10}

\bibitem{AU1972}
Alfred H.~Aho and Jeffrey D.~Ullman.
\newblock The Theory of Parsing, Translation, and Computing
\newblock Prentice-Hall, Englewood Cliff, N.J., 1972.

\bibitem{AH2002}
John~Aycock and R.~Nigel~Horspool.
\newblock Practical Earley Parsing
\newblock {\em The Computer Journal},
    Vol. 45, No. 6, 2002, pp. 620-630.

\bibitem{Earley1970}
J.~Earley.
\newblock An efficient context-free parsing algorithm.
\newblock {\em Communications of the Association for Computing Machinery},
  13(2):94--102, 1970.

\bibitem{GJ2008}
Dirk~Grune and Ceriel~J.H Jacobs
\newblock {\em Parsing Techniques: A Practical Guide}.
\newblock Springer, Amsterdam, 2008.

\bibitem{Irons}
Edgar~T.~Irons.
\newblock A syntax-directed compiler for ALGOL 60.
\newblock {\em Communications of the Association for Computing Machinery},
 4(1):51-55, Jan. 1961

\bibitem{Marpa-HTML}
Jeffrey~Kegler, 2011: Marpa-HTML.
\newblock [ Available online at \url{http://search.cpan.org/dist/Marpa-HTML/}. ]

\bibitem{Marpa-R2}
Jeffrey~Kegler, 2012: Marpa-R2.
\newblock [ Available online at \url{http://search.cpan.org/dist/Marpa-R2/}. ]

\bibitem{Marpa-XS}
Jeffrey~Kegler, 2011: Marpa-XS-1.002000.
\newblock [ Available online at \url{http://search.cpan.org/dist/Marpa-XS/}. ]

\bibitem{Leo1991}
J.~M. I.~M. Leo.
\newblock A general context-free parsing algorithm running in linear time on
  every {LR($k$)} grammar without using lookahead.
\newblock {\em Theoretical Computer Science}, 82:165--176, 1991.

\end{thebibliography}

\tableofcontents

\end{document}
