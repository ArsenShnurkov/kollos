% Copyright 2012 Jeffrey Kegler
% This document is licensed under
% a Creative Commons Attribution-NoDerivs 3.0 United States License.
\documentclass[12pt]{amsart}
\usepackage{algorithm}
\usepackage{algpseudocode}

\newcommand{\comment}[1]{}
\newcommand{\myspace}{\mbox{ }\ \ \ \ }
\newcommand{\myspacem}{\;\;\;\;\;}
\newcommand{\sep}{\,\mid\,}
\newcommand{\mydot}{\raisebox{.15em}{\tiny $\,\bullet\,$}}
\newcommand{\size}[1]{\left | {#1} \right |} 
\newcommand{\order}[1]{{\mathcal O}(#1)}

\newcommand{\var}[1]{\ensuremath{\textbf{#1}}}

\newcommand{\cfg}{CFG}

\newcommand{\de}{\rightarrow}
\newcommand{\derivg}[1]{\mathrel{\mbox{$\:\Rightarrow\:$}}}
\newcommand{\derivrg}[2]{\mathrel{\mbox{$\:\stackrel{\!{#1}}%
        {\Rightarrow\!}\:$}}}

\newcommand{\ep}{\varepsilon}

\newcommand{\set}[1]{{\lbrace #1 \rbrace} }
\newcommand{\ah}[1]{#1_{AH}}
\newcommand{\Vah}[1]{\ensuremath{\var{#1}_{AH}}}
\newcommand{\dr}[1]{#1_{DR}}
\newcommand{\Vdr}[1]{\ensuremath{\var{#1}_{DR}}}
\newcommand{\eim}[1]{#1_{EIM}}
\newcommand{\Veim}[1]{\ensuremath{\var{#1}_{EIM}}}
\newcommand{\Veimt}[1]{\ensuremath{\var{#1}_{EIMT}}}
\newcommand{\Veimx}[1]{\ensuremath{\var{#1}_{EIMX}}}
\newcommand{\Veimm}[1]{\ensuremath{\var{#1}_{EIMM}}}
\newcommand{\Veimset}[1]{\ensuremath{\var{#1}_{\set{EIM}}}}
\newcommand{\Veimtset}[1]{\ensuremath{\var{#1}_{\set{EIMT}}}}
\newcommand{\Veimxset}[1]{\ensuremath{\var{#1}_{\set{EIMX}}}}
\newcommand{\Veimmset}[1]{\ensuremath{\var{#1}_{\set{EIMM}}}}
\newcommand{\es}[1]{#1_{ES}}
\newcommand{\loc}[1]{#1_{LOC}}
\newcommand{\Vrule}[1]{\ensuremath{\var{#1}_{RULE}}}
\newcommand{\sym}[1]{#1_{SYM}}
\newcommand{\Vsym}[1]{\ensuremath{\var{#1}_{SYM}}}
\newcommand{\symset}[1]{#1_{\lbrace SYM \rbrace} }
\newcommand{\term}[1]{#1_{TERM}}
\newcommand{\token}[1]{#1_{TOK}}
\newcommand{\alg}[1]{\ensuremath{\textsc{#1}}}
\newcommand{\AH}{\ensuremath{\alg{AH}}}
\newcommand{\Earley}{\ensuremath{\alg{Earley}}}
\newcommand{\Leo}{\ensuremath{\alg{Leo}}}
\newcommand{\Marpa}{\ensuremath{\alg{Marpa}}}
\newcommand{\Emulator}{\ensuremath{\alg{Emulator}}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem*{definition}{Definition}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\DeclareMathOperator{\scanned}{scanned}
\DeclareMathOperator{\Hyp}{Hyp}
\DeclareMathOperator{\GOTO}{GOTO}
\newcommand\myL[1]{\operatorname{L}(#1)}
\newcommand\tables[1]{\es{\operatorname{table}(#1)}}

% I want to use 'call' outside of pseudocode
\newcommand\call[2]{\textproc{#1}\ifthenelse{\equal{#2}{}}{}{(#2)}}%

\hyphenation{ALGOL}

\begin{document}

\title{Marpa, a practical general parser: the recognizer}

\author{Jeffrey Kegler}
\thanks{
Copyright \copyright\ 2012 Jeffrey Kegler.
This document is licensed under
a Creative Commons Attribution-NoDerivs 3.0 United States License.
}

\begin{abstract}
This document reports describes the recognizer portion
of the Marpa algorithm.
The Marpa algorithm is a practical, and fully implemented,
algorithm for the recognition,
parsing and evaluation of context-free grammars.
Marpa's recognizer is based
an Earley's algorithm,
and merges the improvements to Earley's
from Leo~\cite{Leo1991}
and Aycock and Horspool~\cite{AH2002}.
Their combination in the marpa parse engine
has an added feature:
It makes available,
before each token is scanned,
full knowledge of the state of the parse so far.
This allows input to be altered as the parse progresses,
which can be an extremely powerful technique.
\end{abstract}

\maketitle

\section{THIS IS AN EARLY DRAFT}

This a very early draft of this document,
and is largely unchecked.
The spirit and culture of the open source community
dictates that works of this kind and
at even this very early stage be made
available on-line in public repositories.
The reader should see the availability of this document
as compliance with that spirit and culture,
and not as in any way suggesting or
encouraging reliance on its contents.

\section{Introduction}

Despite the promise of general context-free parsing,
and the strong academic literature behind it,
it has never been incorporated into a tool
as widely available as yacc or
regular expressions.
The Marpa project was begun to end this neglect by
pulling the best results from the literature,
and to turning them into a widely-available tool.
A stable version of this tool, Marpa::XS~\cite{Marpa-XS},
was uploaded to the CPAN Perl archive
on Solstice Day in 2011.

Marpa::XS is a complete implementation of a parser
generator.
Its recognizer is built around a parse engine,
which is new, but which also owes a great debt
to previous work.

\section{Preliminaries}
\label{s:prel}

I assume familiarity with standard grammar notation
(pages 14-15 in Aho and Ullman~\cite{AU1972}).
In the past,
the type system required to support
a theory of parsing has been taken
as a challenge to the typographic
imagination,
and often become one to the eyesight.
This document will 
often use subscripts to indicate the commonly occurring types.
For example, $\sym{a}$ and $\sym{X}$ will be symbols.

Where $Abc$ is a set of symbols,
let $Abc^\ast$ be the set of all strings formed
from those symbols.
Let $Abc^+$ be the subset of $Abc^\ast$ that
contains all of its elements that are not of zero length.

For the purposes of this document consider,
without loss of generality,
a grammar $g$,
and a set of symbols, $alphabet$.
Call the language of \var{g}, $\myL{g}$,
where $\myL{g} \in alphabet^\ast$
Let its input be \var{w}, $\var{w} \in \var{alphabet}^\ast$.
Divide $alphabet$ into two disjoint sets,
$lh$ and $term$.

For the rewriting, designate a set of duples, $rules$,
where $\forall Rule \sep Rule \in rules$,
$Rule$ takes the form $\sym{l} \de r$,
where $\sym{l} \in term$ and 
$r \in (term \cup lh)^+$. 
$\sym{l}$ is referred to as the left hand side (LHS)
of $Rule$.
$r$ is referred to as the right hand side (RHS)
of $Rule$.
This definition differs from the traditional one in
that the empty RHS is not allowed.
Let one symbol \Vsym{start}, $\Vsym{start} \in lh$,
be a dedicated start symbol.

The grammar $g$ can be defined as the 4-tuple
$(lh, term, rules, \sym{start})$.
Without loss of generality,
it is assumed that $g$ is "augmented",
so that there is a rule $\Vsym{start} \de \Vsym{old-start}$,
and that \Vsym{start} is not the LHS of any other rule,
or in the RHS of any rule.

We have already noted
that no rules of \var{g}
have a zero-length RHS.
Further, no symbol may be a proper nullable --
all symbol must be either nulling or non-nullable.
These restrictions follow Aycock and Horspool~\cite{AH2002}.

Aycock and Horspool did allow a single empty start rule
to deal with null parses.
Marpa completely eliminates the need for empty rules in its grammars
by treating null parses and trivial grammars as special cases.
(Trivial grammars are those which recognize only the null string.)

The elimination of empty rules and proper nullables
is done by rewriting the grammar.
This can be done without loss of generality
and, in their paper~\cite{AH2002},
Aycock and Horspool
show how to do this
without effect on the time complexity
as a function of the input size.
Very importantly,
this rewrite is done in such a way that the semantics
of the original grammar can be efficiently reconstructed
at evaluation time.

In this document, $Earley$ will refer to the Earley's original
recognizer~\cite{Earley1970}.
$Leo$ will refer to Leo's revision of $Earley$
as described in his 1991 paper~\cite{Leo1991}.
$AH$ will refer to the Aycock and Horsool's revision
of $Earley$
as described in their 2002 paper~\cite{AH2002}.
Where $\alg{Recce}$ is a recognizer,
$\myL{\alg{Recce},g}$ will be the language accepted by $\alg{Recce}$
when parsing grammar $g$.

\section{The AHFA Finite Automatio}
\label{s:AHFA}

In this document a
"split LR(0) $\epsilon$-DFA"
a described by Aycock and Horspool~\cite{AH2002},
will be called an Aycock-Horspool Finite Automaton,
or AHFA.
A full description of how to derive an AHFA in theory
can be found in \cite{AH2002},
and examples of how to derive it in practice
can be found in the code for Marpa\cite{Marpa-RS,Marpa-XS}.
Here I will summarize those ideas behind AHFA's
that are central to Marpa.

Aycock and Horspool based their AHFA's
on a few observations.
\begin{itemize}
\item
In practice, Earley items with the same dotted rule
often appear in groups in the Earley sets,
where the entire shares the same origin.
\item
There was already in the literature a method
for associating groups of dotted rules that often appear together
when parsing.
This method was the LR(0) DFA used in the much-studied
LALR and LR parsers.
\item
The LR(0) items that are the components of LR(0)
states are, exactly, dotted rules.
\item
By taking into account symbols which derive the
null string, the LR(0) DFA could be turned into an
LR(0) $\epsilon$-DFA, which would be even more effective
at grouping dotted rules which occur together.
\end{itemize}

AHFA states are sets of dotted rules.
Aycock and Horspool realized that by changing Earley items
to track AHFA states, instead of individual dotted rules,
the size of Earley sets could be reduced,
and Earley's algorithm made faster in practice.
In short, then, AHFA states are a shorthand that Earley items
can use for groups of dotted rules that occur together frequently.
The original Earley items could be represented as $(\Vdr{r}, origin)$
duples, where \Vdr{r} is a dotted rule.
Aycock and Horspool modified their Earley items to be $(\ah{L}, origin)$
duples, where $\ah{L}$ is an AHFA state.

\begin{definition}
A dotted rule is {\bf Marpa-valid} if and only if
it does not have a nulling postdot symbol.
\end{definition}

It is important to note that
an AHFA is not a partition of the dotted
rules --
a single dotted rule can occur
in more than one AHFA state.
This does not happen frequently,
but it does happens often enough,
even in practical grammars,
that the Marpa implementation has to provide for it.

What does not seem to happen in practical grammars
the size of \AH Earley set to grow larger
than that of one of Earley's original sets.
That is, it seems the AHFA is always a win,
at least for practical grammars.

Use of the AHFA states in EIM's can
be proved to be break-even for
time and space complexity purposes.
The next observation is useful for that purpose.

\begin{lemma}
Let $g$ be a grammar as defined in this
document,
and let $fa$ be the AHFA for this grammar.
Let $\var{S-t}$ be
Earley set $i$ of a traditional earley parse.
Let $\var{S-m}$ be an set
of Earley items of the Aycock-Horspool form (EIMA),
such that
\begin{gather*}
\forall r, origin
\exists \ah{state} \sep \\
[\Vdr{r}, origin] \in \var{S-t} \\
\implies
\ah{state} \in fa
\land \Vdr{r} \in \ah{state}
\land [\ah{state}, origin] \in \var{S-m}
\end{gather*}
Then 
$\size{\var{S-m}} < c \times \size{\var{S-t}}$,
where $c$ is the number of states in $fa$.
\end{lemma}

\begin{proof}
The worst case is that every rule of 
$\var{S-t}$ has a different $origin$,
Worst case for grouping dotted rules into $c$ unique states
is that every dotted rule appears in $c - 1$ states.
$\size{\var{S-t}} < c \times \size{\var{S-m}}$.
\end{proof}

\section{The Leo algorithm}

\begin{theorem}
If the AHFA state of
a Marpa Earley item (EIMM) is the result of a
Leo completion,
then its AHFA state contains only one dotted rule.
\end{theorem}

\begin{proof}
Since the Earley item is the result of a Leo completion,
we know that its AHFA state contains a completed rule.
Call that completed rule, \Vdr{complete}.
Let \Vrule{c} be the rule of \Vdr{complete},
and \var{cp} its dot position.
$\var{cp} > 0$ because completions are never
predictions.

Suppose, for a reduction to absurdity,
and that the AHFA state contains another dotted rules,
\Vdr{other},
$\Vdr{complete} \neq \Vdr{other}$.
Let \Vrule{o} be the rule of \Vdr{other},
and \var{op} its dot position.
An AHFA construction never places predictions in the same
AHFA state as a completion, so
\Vdr{other} is not a prediction.
Therefore, $\var{op} > 0$.

To create a contradiction, we first prove that
$\Vrule{c} \neq \Vrule{o}$,
then that 
$\Vrule{c} = \Vrule{o}$.
By the construction of an AHFA
state, both dotted rules resulted from the same series
of transitions.
But the same series of transitions over the
same rule would result in the same dot position,
$\var{cp} = \var{op}$
so that if $\Vrule{c} = \Vrule{o}$,
$\Vdr{complete} = \Vdr{other}$,
which is contrary to the assumption for the reduction.
Therefore, under the assumption for the reduction,
$\Vrule{c} \neq \Vrule{o}$.

Next we show that, under the assumption for the reduction,
that $\Vrule{c} = \Vrule{o}$
follows from Leo's uniqueness requirement.
Since both dotted rules are in the same EIMM
and neither is a predictions,
both must result from transitions,
and their transitions must have been from the same Earley set.
Since they are in the same AHFA state,
by the AHFA construction,
that transition must have been
over the same transition symbol.
Leo requires that, for each predecessor Earley set and transition symbol,
that the transition be from a single rule.
The assumption for the reduction is that \Vdr{complete}
and \Vdr{other} are in the same Leo completion state,
and in order for them to have obeyed the Leo uniqueness
requirement,
we have $\Vrule{c} = \Vrule{o}$.

We now have both
$\Vrule{c} = \Vrule{o}$
and 
$\Vrule{c} \neq \Vrule{o}$,
completing the reduction to absurdity.
When \Vdr{complete} is in a Leo completion EIMM,
it must be the only dotted rule in that EIMM.
\end{proof}

\section{The Marpa Recognizer}
\label{s:recce}

\begin{algorithm}[H]
\caption{Marpa Initialization}\label{a:initial}
\begin{algorithmic}[1] 
\Procedure{Initial}{$i,a$}
\State \Call{AddItems}{$0, \ah{start}, 0$}
\EndProcedure
\end{algorithmic} 
\end{algorithm}

\begin{algorithm}[H]
\caption{Marpa Scanning}\label{a:scan}
\begin{algorithmic}[1] 
\Procedure{Scan}{$i,a$}
\Comment{Scan a token $a$, which starts at Earley set $i$}
\For{each Earley item $\eim{x}$ in $ES[i]$}
\State $\ah{to} \gets \GOTO(\operatorname{AHFA-of}(\eim{x}), a)$
\State \Call{Add EIM Pair}{$i+1, \ah{to}, \operatorname{Origin-of}(\eim{x}$}
\EndFor
\EndProcedure
\end{algorithmic} 
\end{algorithm}

\begin{algorithm}[H]
\caption{Marpa Completion}\label{a:complete}
\begin{algorithmic}[1] 
\Procedure{Complete}{$i_{LOC}$}
\For{each Earley item $\eim{work} \in ES[i]$}
\For{each completed rule, $\Vrule{C}$}
\For{each $\eim{predecessor} \in ES[\operatorname{Origin-of} \eim{work}]$}
\State $\ah{to} \gets \GOTO(\operatorname{AHFA-of} \eim{predecessor}, LHS_{SYM})$
\State \Call{Add EIM Pair}{$to_{AHFA}, \operatorname{Origin-of} \eim{predecessor}$}
\EndFor
\EndFor
\EndFor
\State Construct $transitions[i]$
\EndProcedure
\end{algorithmic} 
\end{algorithm}

\begin{algorithm}[H]
\caption{Adding EIM Pairs}\label{a:pair}
\begin{algorithmic}[1] 
\Function{Add EIM Pair}{$i_{LOC},\ah{discovered},origin_{LOC}$}
\State $\ah{predicted} \gets \Lambda$
\State $\eim{discovered} \gets \Lambda$
\If{$\ah{to} \neq \Lambda$}
\State $\eim{discovered} \gets [\ah{to}, origin_{LOC}]$
\If{$\eim{discovered}$ is new}
\State Add $\eim{discovered}$ to $ES[i]$
\State $\ah{predicted} \gets \GOTO(\ah{to}, \epsilon)$
\EndIf
\EndIf
\If{$predicted_{AHFA} \neq \Lambda$}
\State $\eim{predicted} \gets [\ah{predicted}, i]$
\If{$eim{predicted}$ is new}
\State Add $\eim{predicted}$ to $ES[i]$
\EndIf
\EndIf
\State \textbf{Return} $\eim{discovered}$
\EndFunction
\end{algorithmic} 
\end{algorithm} 

\section{Marpa recognizer correctness}
\label{s:marpa:correctness}

As mentioned, Marpa grammars are 
rewritten to eliminate proper nullable
symbols and empty rules.
adn the AHFA states collect dotted rules
which always appear together.
Some AHFA states ("kernel states")
have an $\epsilon$ transition
to one other AHFA state (a "non-kernel" state).
A non-kernel state never has an $\epsilon$ transition,
so and the $\epsilon$ transitions can be seen
as defined a partition of AHFA states into
sets of cardinality 1 or 2.

\subsection{Proof strategy}

Marpa's correctness will shown based the correctness
proved for \Leo in~\cite{Leo1991}.
An inductive proof with recognizer operations as steps,
but in order for the induction to work,
the operations of \Marpa and \Leo must be dealt with
as if they occurred in the same order.
This will be accomplished by constructing a third
recognizers, \alg{Emulator}, which will,
in effect, emulate \Marpa{} using \Leo{}.
of operations.

\subsection{The emulator}

We define a new algorithm,
\alg{Emulator}.
\Emulator will be \Leo,
with the following modifications.

\subsubsection{Calculating the AHFA}

\alg{Emulator}, as part of its precomputations
on its grammar, \var{g},
will calculate \var{ah}, the AHFA
It will also calculate $\GOTO: \var{ah},\var{alphabet} \mapsto \var{ah}$,
the transition function for \var{ah}.

\subsubsection{Extending the Earley items}

The Earley sets of \Emulator{} will contain extended
Earley items, EIMX's.  Each EIMX will have the form
$$[\Vah{state}, \Vdr{dr}, \var{origin}]$$
where \Vah{state} is an AHFA state,
\Vdr{dr} is a dotted rule,
\var{origin} is the origin.
EIMX's can be viewed as
crosses ("X") between traditional Earley items (EIMT's)
and Marpa's Earley items (EIMM's).

A set of EIMX's (\Veimset{X}) and a set of EIMT's
(\Veimset{T})
are congruent ($\Veimset{X} \cong \Veimset{T})$)
if and only if
for all
\Vah{x}, \Vdr{x}, \var{xorig}
\begin{equation*}
\Vah{x}, \Vdr{x}, \var{xorig} \in \Veimxset{X} \iff
[\Vdr{t}, \var{torig}] \in \Veimtset{T}
\end{equation*}

\subsubsection{The emulation engine}

For scanning, discovery and prediction,
\Emulator follows
uses the \Leo{} logic.
We define the \Emulator so that it look over each
Earley set repeatedly,
until either no new EIMX's can be added.
EIMX's are never added if they are duplicates,
and the number of possible EIMX's for a given Earley set
is finite,
so that we know the loop will terminate.

In dealing with EIMX's as predecessors
and causes,
the \Leo logic
ignores the AHFA state and
treats them as if they were EIMT's.
In similar fashion,
when dealing with the extended Leo item's,
to be described below,
the \Leo logic
ignores the AHFA state and
treats them as if they had the form they have in \Leo.
By applying the \Leo{} logic in this manner,
\Emulator calculates a new traditional Earley item,
\Veimt{new}.

\Emulator{} next determines \Vah{new},
and \Vah{predict}
two AHFA statess,
as follows:
\begin{itemize}
\item
If it is scanning \Vsym{token},
a non-nullable token,
$Vah{new} = \GOTO{\Vah{predecessor}, \Vsym{token}}$,
where 
where \Vah{predecessor} is the AHFA state of the predecessor EIMX.
If it is scanning an and \Vsym{token}
is nulling, $\Vah{new} = \Vah{predecessor}$.
\Vah{predict} is set to $\GOTO(\Vah{predecessor}, \epsilon)$
\item
If it is discovering, and Leo discovery does not
apply, then
$Vah{new} = \GOTO{\Vah{predecessor}, \Vsym{lhs}}$,
where \Vah{predecessor} is the AHFA state of the predecessor EIMX.
and \Vsym{lhs} is the LHS of the completed rule.
\item
If it is discovering, and Leo discovery does apply,
$Vah{new} = \Vah{leo}$, where \Vah{leo} is
the state in the extended Leo item.
Leo discovery in the \Emulator{}
will be described below.
\Vah{predict} is set to $\GOTO(\Vah{predecessor}, \epsilon)$
\item For predictions, $\Vah{new} = \Lambda$.
Call \Vah{cause}, the AHFA state of the EIMX that caused
the prediction.
If \Vah{cause} is itself a prediction, $\Vah{predict} = \Vah{cause}$.
If \Vah{cause} is not a prediction, $\Vah{predict} = \GOTO(\Vah{cause}, \epsilon)$.
\end{itemize}

if $\Vah{new} \neg \Lambda$,
\Emulator produces the \Veimtset{discovered}, where
\begin{equation}
\Veimtset{discovered}  =
{ [ \Vah{new}, \Vdr{dr}, \var{origin} ] \sep \Vdr{dr} \in \epsilon(\Vah{new}) }
\end{equation}
where \var{origin} is the origin of \Veimt{new},
and 
$\epsilon(\Vah{new})$ is the $\epsilon$-closure of the dotted rules in \Vah{new}.
The EIMX's from 
\Veimtset{discovered} are added to the current Earley set
in Emulator Dotted Rule Order.
Emulator Dotted Rule Order is some fixed, total
ordering,
which must order completed rules
in Emulator Dotted Symbol Order by LHS.
(The order must be fixed
so that that it can be
shared with \Marpa{}.)

Next, if $\Vah{predict} neg \Lambda$,
\Emulator produces,
for every EIMX in the $\epsilon$-closure of \Vah{new},
${ [ \Vah{predict}, \Vdr{dr}, \var{current} ] \sep \Vdr{dr} \in \epsilon(\Vah{predict}) }$,
where \var{current} is the current Earley set.
The EIMX's from 
Veimtset{predicted} are added to the current Earley set
in Emulator Dotted Rule Order.


\subsubsection{Initialization in the Emulator}

It is convenient to treat \Emulator{}
initialization is handled as a special case.
To build Earley set 0, \Emulator{}
first adds the set
\begin{equation}
{ [ \Vah{start}, \Vdr{dr}, 0 ] \sep \Vdr{dr} \in \epsilon(\Vah{start}) }
\end{equation}
in Dotted Rule Order.
It then examines \Vah{prediciton},
where $\Vah{prediction} = \GOTO(\Vah{cause}, \epsilon)$.
If $\Vah{prediction} \neq \Lambda$,
Emulator add the set
\begin{equation}
{ [ \Vah{predicted}, \Vdr{dr}, 0 ] \sep \Vdr{dr} \in \epsilon(\Vah{predicted}) }
\end{equation}
in Emulator Dotted Rule Order.

\subsubsection{Changes to the Leo items}

Leo items (LIM's) will also be extended.
The extended Leo items will take the form
$$[\Vah{state}, \Vdr{dr}, \Vsym{trans}, \var{origin}]$$
where 
$$[\Vdr{dr}, \Vsym{trans}, \var{origin}]$$ is the original
Leo item and 
\Vah{state} will be the state of the Leo discovery item
that will be added when the Leo item is used.
Unlike \Leo,
\Emulator compute the LIMX's on an eager basis,
immediately
after each Earley set is finished.
This is possible because there is enough information
in the Earley set to identify potential LIM's.

\section{The emulator is correct}

We now show that,
for a given grammar \var{g} and input \var{w},
the Earley sets of \Emulator{} are always congruent
to those of \Leo.

\begin{theorem}
Let $\operatorname{L}[i]$
be Earley set $i$ in \Leo,
and $\operatorname{E}[i]$
be Earley set $i$ in \Emulator.
\begin{equation*}
\forall i \sep \operatorname{L}[i] \cong
\operatorname{E}[i]
\end{equation*}
\end{theorem}

\begin{proof}
The proof proceeds by examining the modifications
to \Leo{} for \Emulator{} one or two at a time,
confirming that congruence at each point.
We first examine initialization and the
addition of EIMX's to the Earley sets.
It is straightforward to confirm, as the basis
of an induction, that 
$$ \operatorname{L}[0] \cong \operatorname{E}[0]$$.

\Emulator{} is defined in terms of the logic
of \Leo{}.
Suppose that,
at the beginning of
every operation of the emulation engine,
the Earley sets produced by \Emulator were congruent to \Leo.
The emulation engine will treat the EIMX's as traditional EIMT's,
and the LIMX's as Leo items,
and will produce the a new traditional Earley item \Veimt{new}.
Clearly, if \Leo and \Emulator are operating side by side,
\Veimt{new} is also the item that \Leo produces.
It is also clear that \Emulator one of the EIMX's that \Emulator attempts
to add to its Earley sets
is \Veimx{new}, the EIMX that is congruent to \Veimt.
\Emulator will only fail to fail to add 
\Veimx{new} if it already exists in the Earley set.

\end{proof}

\section{Marpa recognizer time complexity}
\label{s:marpa:complexity}

\begin{theorem}\label{t:constant-per-EIM}
All time in \Marpa{} can be allocated
to the Earley items,
and in such a way that processing each Earley item
requires $\order{1}$ time.
\end{theorem}

\begin{proof}
It is known that the time taken by
the traditional Earley recognizer
for its operations
operations can be done in amortized $\order(1)$
time per Earley item~\cite[Vol. 1, pages 326-327]{AU1972}.
Inspection of the algorithm for \Marpa{} will show
that any operations considered to be new to it
can be assigned to the Earley items in
an obivous way,
and 
trivially shown to be $\order{1}$.
\end{proof}

\begin{theorem}
For all context-free grammars,
the time and space complexity
of \Marpa{} is as good or better
than that of Earley's algorithm.
\end{theorem}

\begin{proof}
The time and space complexity
results for $\Earley$ are based
on establishing the order of the number of Earley items,
and showing that $\Earley$ takes
$\order{1}$ time and 
$\order{1}$ space
for each Earley
item.~\cite[Vol. 1, pages 325-327]{AU1972}.
By Theorem \ref{a:eim-order},
for any grammar, the order of the number of Earley
items is the same in \Marpa{} as it is
in $\Earley$.
It is trivial to show that the space requirements for
\Marpa{} are $\order{1}$ time for each Earley item.
By Theorem ref{t:constant-per-EIM},
the time requirements for
\Marpa{} are also $\order{1}$ for each Earley item.
Therefore the time and space complexity results for $\Earley$
hold for \Marpa.
\end{proof}

\begin{theorem}
For all context-free grammars,
the time and space complexity
of \Marpa{} is as good or better
than that of Earley's algorithm.
\end{theorem}

\begin{theorem}
For every LR-regular grammar,
\Marpa{} runs in linear time and linear space.
\end{theorem}

\section{Generalizing the grammar}
\label{s:generalizing}

This section deals with certain modifications
to the
definition of a grammar made by Marpa.
They
are minor from a theoretical point of view,
and their discussion has been deferred so that the
proofs up to this point might more closely follow
tradition.

First, Marpa does not require that $\symset{lh}$
and $\symset{lh}$ be disjoint.
In other words a Marpa symbol may be both a terminal
and a LHS.
This expansion of the grammar definition 
is made without loss of generalization,
or effect on the results so far.

Second, Marpa's input model is a generalization of
the traditional input stream model
used so far.
Marpa's input is a set of tokens,
$toks$,
whose elements are triples of symbol,
start location and end location:
$(\sym{t}, \loc{start}, length)$
such that $\sym{t} \in \symset{lh} \wedge length \ge 1 \wedge \loc{start} \ge 0$.
The size of the input, $\size{toks}$ is the maximum over
$toks$ of $\loc{start}+length$.
Tokens may overlap,
but gaps are not allowed:
$\forall \loc{i} (\exists \token{t} \sep \token{t} \in toks
\wedge t = (\sym{t}, \loc{start}, length)
\wedge \loc{start} \le \loc{i} \le \loc{start}+length$.

The traditional input stream is the special case of
a Marpa input stream where 
$\forall \token{tok} \sep \token{tok} \in toks \implies 
\token{tok} = (\sym{s}, \loc{start}, 1)
$
and
$\forall \token{tok1}, \token{tok2} \in toks \sep
\token{tok1} = (\sym{s1}, \loc{start1}, 1)
\wedge
\token{tok2} = (\sym{s2}, \loc{start2}, 1)
\wedge (\loc{start1} = \loc{start2} \implies \token{tok1} = \token{tok2})
$.

The correctness results hold for Marpa input streams,
but to preserve the time complexity bounds,
certain restriction must be imposed on them.
Call an Earley set, $\es{table[j]}$
a "lookahead set" at $\loc{i}$
if $j>i \wedge$
and there exists some token $\token{tok} = (\sym{s}, \loc{start}, length)$
such that $\loc{start} \le i \wedge \loc{start}+length = \loc{j}$.
If, at every location,
the number of tokens which start there,
and the number of lookahead sets in play at that location
is less than a finite constant,
then Earley items can be added in amortized constant time
and the complexity results for
\Marpa{} stand.

\bibliographystyle{plain}

\begin{thebibliography}{10}

\bibitem{AU1972}
Alfred H.~Aho and Jeffrey D.~Ullman.
\newblock The Theory of Parsing, Translation, and Computing
\newblock Prentice-Hall, Englewood Cliff, N.J., 1972.

\bibitem{AH2002}
John~Aycock and R.~Nigel~Horspool.
\newblock Practical Earley Parsing
\newblock {\em The Computer Journal},
    Vol. 45, No. 6, 2002, pp. 620-630.

\bibitem{Earley1970}
J.~Earley.
\newblock An efficient context-free parsing algorithm.
\newblock {\em Communications of the Association for Computing Machinery},
  13(2):94--102, 1970.

\bibitem{GJ2008}
Dirk~Grune and Ceriel~J.H Jacobs
\newblock {\em Parsing Techniques: A Practical Guide}.
\newblock Springer, Amsterdam, 2008.

\bibitem{Marpa-R2}
Jeffrey~Kegler, 2012: Marpa-R2.
\newblock [ Available online at http://search.cpan.org/dist/Marpa-R2/. ]

\bibitem{Marpa-XS}
Jeffrey~Kegler, 2011: Marpa-XS-1.002000.
\newblock [ Available online at http://search.cpan.org/dist/Marpa-XS/. ]

\bibitem{Leo1991}
J.~M. I.~M. Leo.
\newblock A general context-free parsing algorithm running in linear time on
  every {LR($k$)} grammar without using lookahead.
\newblock {\em Theoretical Computer Science}, 82:165--176, 1991.

\end{thebibliography}
 
\end{document}
