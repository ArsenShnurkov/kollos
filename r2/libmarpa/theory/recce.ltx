% Copyright 2012 Jeffrey Kegler
% This document is licensed under
% a Creative Commons Attribution-NoDerivs 3.0 United States License.
\documentclass[12pt]{amsart}
\usepackage{algorithm}
\usepackage{algpseudocode}

\newcommand{\comment}[1]{}
\newcommand{\myspace}{\mbox{ }\ \ \ \ }
\newcommand{\myspacem}{\;\;\;\;\;}
\newcommand{\sep}{\,\mid\,}
\newcommand{\mydot}{\raisebox{.15em}{\tiny $\,\bullet\,$}}
\newcommand{\size}[1]{\left | {#1} \right |} 
\newcommand{\order}[1]{{\mathcal O}(#1)}

\newcommand{\var}[1]{\ensuremath{\textbf{#1}}}

\newcommand{\cfg}{CFG}
\newcommand{\nonterm}{\mbox{$V_{{\rm N}}$}}
\newcommand{\myterm}{\mbox{$V_{{\rm T}}$}}

\newcommand{\de}{\rightarrow}
\newcommand{\derivg}[1]{\mathrel{\mbox{$\:\Rightarrow\:$}}}
\newcommand{\derivrg}[2]{\mathrel{\mbox{$\:\stackrel{\!{#1}}%
        {\Rightarrow\!}\:$}}}

\newcommand{\ep}{\varepsilon}

\newcommand{\ah}[1]{#1_{AH}}
\newcommand{\aim}[1]{#1_{AIM}}
\newcommand{\eim}[1]{#1_{EIM}}
\newcommand{\es}[1]{#1_{ES}}
\newcommand{\loc}[1]{#1_{LOC}}
\newcommand{\production}[1]{#1_{RULE}}
\newcommand{\sym}[1]{#1_{SYM}}
\newcommand{\symset}[1]{#1_{\lbrace SYM \rbrace} }
\newcommand{\term}[1]{#1_{TERM}}
\newcommand{\token}[1]{#1_{TOK}}
\newcommand{\alg}[1]{\textsc{#1}}
\newcommand{\AH}{\ensuremath{\alg{AH}}}
\newcommand{\Earley}{\ensuremath{\alg{Earley}}}
\newcommand{\Leo}{\ensuremath{\alg{Leo}}}
\newcommand{\Marpa}{\ensuremath{\alg{Marpa}}}
\newcommand{\Rubyslippers}{\ensuremath{\alg{Ruby-Slippers}}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\DeclareMathOperator{\scanned}{scanned}
\DeclareMathOperator{\Hyp}{Hyp}
\newcommand\myL[1]{\operatorname{L}(#1)}
\newcommand\tables[1]{\es{\operatorname{table}(#1)}}

% I want to use 'call' outside of pseudocode
\newcommand\call[2]{\textproc{#1}\ifthenelse{\equal{#2}{}}{}{(#2)}}%

\hyphenation{ALGOL}

\begin{document}

\title{Marpa, a practical general parser: the recognizer}

\author{Jeffrey Kegler}
\thanks{
Copyright \copyright\ 2012 Jeffrey Kegler.
This document is licensed under
a Creative Commons Attribution-NoDerivs 3.0 United States License.
}

\begin{abstract}
This paper reports describes the recognizer portion
of the Marpa algorithm.
The Marpa algorithm is a practical, and fully implemented,
algorithm for the recognition,
parsing and evaluation of context-free grammars.
Marpa's recognizer is based
an Earley's algorithm,
and merges the improvements to Earley's
from Leo~\cite{Leo1991}
and Aycock and Horspool~\cite{AH2002}.
Their combination in the marpa parse engine
has an added feature:
It makes available,
before each token is scanned,
full knowledge of the state of the parse so far.
This allows input to be altered as the parse progresses,
which can be an extremely powerful technique.
\end{abstract}

\maketitle

\section{THIS IS AN EARLY DRAFT}

This a very early draft of this paper,
and is largely unchecked.
The spirit and culture of the open source community
currently dictates that works of this kind and at
this very early stage be made
available on-line in public repositories.
The reader should see the availability of this document
as compliance with that spirit and culture,
and not as in any way suggesting or
encouraging reliance on its contents.

\section{Introduction}

Despite the promise of general context-free parsing,
and the strong academic literature behind it,
it has never been incorporated into a tool
as widely available as yacc or
regular expressions.
The Marpa project was begun to end this neglect by
pulling the best results from the literature,
and to turn them into a widely-available tool.
A stable version of this tool, Marpa::XS~\cite{Marpa-XS},
was uploaded to the CPAN Perl archive
on Solstice Day in 2011.

Marpa::XS is a complete implementation of a parser
generator.
Its recognizer is built around a parse engine,
which is new, but which also owes a great debt
to previous work.

\section{Preliminaries}
\label{s:prel}

I assume familiarity with standard grammar notation
(pages 14-15 in Aho and Ullman~\cite{AU1972}).
In the past,
The type system required to support
a theory of parsing has been taken
as a challenge to the typographic
imagination,
and often resulted in one to the eyesight.
This paper will often
supplement the standard notation,
using subscripts to indicate the commonly occurring types.
In general, variable will contain a capital
letter, constants will be all lower case.
For example, $\sym{a}$ and $\sym{b}$ would be symbol constants,
while $\sym{X}$ will be a variable whose value is a symbol.

Where $ABC$ is a set of symbols,
let $ABC^\ast$ be the set of all strings formed
from those symbols.
Let $ABC^+$ be the subset of $ABC^\ast$ that
contains all of its elements that are not of zero length.

For the purposes of this paper consider,
without loss of generality,
a grammar $g$,
and a set of symbols, $alphabet$.
Call the language of $g$, $\myL{g}$,
where $\myL{g} \in alphabet^\ast$
Let its input be $w$, $w \in alphabet$.
Divide $alphabet$ into two disjoint sets,
$term$ and $nonterm$.

For the rewriting, designate a set of duples, $rules$,
where $\forall Rule \sep Rule \in rules$,
$Rule$ takes the form $\sym{l} \de r$,
where $\sym{l} \in nonterm$ and 
$r \in (nonterm \cup term)^+$. 
$sym{l}$ is referred to as the left hand side (LHS)
of $Rule$.
$r$ is referred to as the right hand side (RHS)
of $Rule$.
This definition differs from the traditional one in
that the empty RHS is not allowed.
Let one symbol $\sym{start}$, $\sym{start} \in nonterm$,
be a dedicated start symbol.

The grammar $g$ can be defined as the 4-tuple
$(term, nonterm, rules, \sym{start})$.
Without loss of generality,
it is assumed that $g$ is "augmented",
so that there is a rule $\sym{start} \de \sym{old-start}$,
and that $\sym{start}$ is not the LHS of any other rule,
or in the RHS of any rule.

I was already noted
that no rules of $g$ is allowed to be empty -- to
have a zero-length RHS.
Further, no symbol may be a proper nullable --
all symbol must be either nulling or non-nullable.
These restrictions follow Aycock and Horspool~\cite{AH2002}.

Aycock and Horspool did allow a single empty start rule
to deal with null parses.
Marpa eliminates all need for empty rules in its grammar
by dealing with null parses as a special case.
Marpa also deals with
trivial grammars (those which recognize only the null string)
as a special case.

The elimination of empty rules and proper nullables
is done by rewriting the grammar.
This can be done without loss of generality
and, In their paper~\cite{AH2002},
Aycock and Horspool
show how to do this
without effect on the time complexity
as a function of the input size.
Very importantly,
this rewrite is done in such a way that the semantics
of the original grammar can be efficiently reconstructed
at evaluation time.

In this paper, $Earley$ will refer to the Earley's original
recognizer~\cite{Earley1970}.
$Leo$ will refer to Leo's revision of $Earley$
as described in his 1991 paper~\cite{Leo1991}.
$AH$ will refer to the Aycock and Horsool's revision
of $Earley$
as described in their 2002 paper~\cite{AH2002}.
Where $\alg{Recce}$ is a recognizer,
$\myL{\alg{Recce},g}$ will be the language accepted by $\alg{Recce}$
when parsing grammar $g$.

\section{The AHFA Finite Automatio}
\label{s:AHFA}

In this paper a
"split LR(0) $\epsilon$-DFA"
a described by Aycock and Horspool~\cite{AH2002},
will be called an Aycock-Horspool Finite Automaton,
or AHFA.
Let FA be the AHFA derived from $G$.
as described in~\cite{AH2002}.

A full description of how to derive an AHFA in theory
can be found in \cite{AH2002},
and examples of how to derive it in practice
can be found in the code for various release of Marpa.
Here I will just summarize the central ideas behind AHFA's.

Aycock and Horspool based their AHFA's
on a few observations.
First, in practice, Earley items with the same dotted rule
often appear in groups in the Earley sets,
where the entire shares the same origin.
Second, there was already in the literature a method
for associating groups of dotted rules that often appear together
when parsing.
This method was the LR(0) DFA used in the much-studied
LALR and LR parsers.
Third, the LR(0) items that are the components of LR(0)
states are, exactly, dotted rules.
Fourth, by taking into account symbols which derive the
null string, the LR(0) DFA could be turned into an
LR(0) $\epsilon$-DFA, which would be even more effective
at grouping dotted rules which occur together.

AHFA states are sets of dotted rules.
Aycock and Horspool realized that by changing Earley items
to track AHFA states, instead of individual dotted rules,
the size of Earley sets could be reduced,
and Earley's algorithm made faster in practice.
In short, then, AHFA states are a shorthand that Earley items
can use for groups of dotted rules that occur together frequently.
The original Earley items could be represented as $(r_{DOTTED}, origin)$
duples, where $r_{DOTTED}$ is a dotted rule.
Aycock and Horspool modified their Earley items to be $(\ah{L}, origin)$
duples, where $\ah{L}$ is an AHFA state.

It is important to note that
an AHFA is not a partition of the dotted
rules --
a single dotted rule can occur
in more than one AHFA state.
This does not happen frequently,
but it does happens often enough,
even in practical grammars,
that the Marpa implementation has to provide for it.

What does not seem to happen in practical grammars
the size of \AH Earley set to grow larger
than that of one of Earley's original sets.
That is, it seems the AHFA is always a win,
at least for practical grammars.

Use of the AHFA states in EIM's can
be proved to be break-even for
time and space complexity purposes.
The next observation is useful for that purpose.

\begin{lemma}
Let $\var{S-t}$ be a set of traditional Earley items
$[\dot{r}, origin]$
at parse location $i$,
Let $\var{S-m}$ be a set of Marpa's Earley items
$[\ah{state}, origin]$
formed by grouping the dotted rules of $\var{S-t}$
into AHFA states.
Then 
$\size{\var{S-t}} < c \times \size{\var{S-m}}$,
where $c$ is the number of AHFA states.
\end{lemma}

\begin{proof}
The worst case is that every rule of 
$\var{S-t}$ has a different $origin$,
Worst case for grouping dotted rules into $c$ unique states
is that every dotted rule appears in $c - 1$ states.
$\size{\var{S-t}} < c \times \size{\var{S-m}}$.
\end{proof}

\section{The Leo algorithms}

\section{The Ruby Slippers Recognizer}
\label{s:recce}

\begin{algorithm}[H]
\caption{Ruby Slippers Initialization}\label{a:rs:initial}
\begin{algorithmic}[1] 
\Procedure{Initial}{$i,a$}
\State \Call{AddItems}{$0, \ah{start}, 0$}
\EndProcedure
\end{algorithmic} 
\end{algorithm}

\begin{algorithm}[H]
\caption{Ruby Slippers Scanning}\label{a:rs:scan}
\begin{algorithmic}[1] 
\Procedure{Scan}{$i,a$}
\Comment{Scan a token $a$, which starts at Earley set $i$}
\For{each Earley item $\eim{x}$ in $ES[i]$}
\State $\ah{to} \gets GOTO(\operatorname{AHFA-of}(\eim{x}), a)$
\State \Call{Add EIM Pair}{$i+1, \ah{to}, \operatorname{Origin-of}(\eim{x}$}
\EndFor
\EndProcedure
\end{algorithmic} 
\end{algorithm}

\begin{algorithm}[H]
\caption{Ruby Slippers Completion}\label{a:rs:complete}
\begin{algorithmic}[1] 
\Procedure{Complete}{$i_{LOC}$}
\For{each Earley item $\eim{work} \in ES[i]$}
\For{each completed rule, $\production{C}$}
\For{each $\eim{predecessor} \in ES[\operatorname{Origin-of} \eim{work}]$}
\State $\ah{to} \gets GOTO(\operatorname{AHFA-of} \eim{predecessor}, LHS_{SYM})$
\State \Call{Add EIM Pair}{$to_{AHFA}, \operatorname{Origin-of} \eim{predecessor}$}
\EndFor
\EndFor
\EndFor
\State Construct $transitions[i]$
\EndProcedure
\end{algorithmic} 
\end{algorithm}

\begin{algorithm}[H]
\caption{Ruby Slippers, Adding EIM Pairs}\label{a:rs:pair}
\begin{algorithmic}[1] 
\Function{Add EIM Pair}{$i_{LOC},\ah{discovered},origin_{LOC}$}
\State $\ah{predicted} \gets \Lambda$
\State $\eim{discovered} \gets \Lambda$
\If{$\ah{to} \neq \Lambda$}
\State $\eim{discovered} \gets [\ah{to}, origin_{LOC}]$
\If{$\eim{discovered}$ is new}
\State Add $\eim{discovered}$ to $ES[i]$
\State $\ah{predicted} \gets GOTO(\ah{to}, \epsilon)$
\EndIf
\EndIf
\If{$predicted_{AHFA} \neq \Lambda$}
\State $\eim{predicted} \gets [\ah{predicted}, i]$
\If{$eim{predicted}$ is new}
\State Add $\eim{predicted}$ to $ES[i]$
\EndIf
\EndIf
\State \textbf{Return} $\eim{discovered}$
\EndFunction
\end{algorithmic} 
\end{algorithm} 

\section{Marpa recognizer correctness}
\label{s:marpa:correctness}

As mentioned, Marpa grammars are 
rewritten to eliminate proper nullable
symbols and empty rules.
adn the AHFA states collect dotted rules
which always appear together.
Some AHFA states ("kernel states")
have an $\epsilon$ transition
to one other AHFA state (a "non-kernel" state).
A non-kernel state never has an $\epsilon$ transition,
so and the $\epsilon$ transitions can be seen
as defined a partition of AHFA states into
sets of cardinality 1 or 2.

\subsection{Proof strategy}

Marpa's correctness will shown based the correctness
proved for \Leo in~\cite{Leo1991}.
An inductive proof with recognizer operations as steps,
but in order for the induction to work,
the operations of \Marpa and \Leo must be dealt with
as if they occurred in the same order.
This will be accomplished by constructing two
recognizers, \alg{Marpa-ordered} and \alg{Leo-ordered}.
These will be variants, respectively, of 
\alg{Marpa} and \alg{Leo},
with a well-defined and identical order
of operations.

Call two recognizers correctness-equivalent if and only
they accept the same languages.
\alg{Marpa-ordered} will be shown to be
correctness-equivalent to \Marpa,
and 
\alg{Leo-ordered} will be shown to be
correctness-equivalent to \Leo.
Then the proof
of correctness-equivalence
between 
\alg{Marpa-ordered} and \alg{Leo-ordered}
will be done,
by induction on the recognizer operations.

\subsection{The ordered Leo recognizer}

We define a new algorithm,
\alg{Leo-ordered} based on \Leo.
Call an EIM-group an set of traditional Earley items which
correspond to a single AH Earley item.
We modify it so that the Earley sets are ordered,
so that all the traditional Earley items
from a single EIM-group occur together in the sequence.
Where a traditional Earley item correspond to more
than one AH Earley item in the same Earley set,
it will occur with the first EIM-group that applies.

For the purpose of this section,
two traditional Earley items are said to {bf co-occur}
if whenever either appears in an AHFA state,
the other also does.

Informally, an EIM-causality is the set of factors
which are necessary to cause the addition of a traditional
Earley item to an Earley set.
The initialization of the Earley table is one such factor.
The other factors are tokens and other Earley items.

\begin{itemize}
\item The EIM-causality of a predicted EIM is a single EIM:
the one which predicts it
by having the predicted EIM's LHS
as a postdot symbol.
\item
The EIM-causality of
a scanned EIM is its token and its predecessor EIM.
The predecessor EIM is the EIM in the previous Earley
set which had the token as its postdot symbol.
\item
The EIM-causality of a completion EIM is a pair of EIM's:
a cause EIM and a predecessor EIM.
Recall that a completion EIM is added as a result of finding
a completed rule in another EIM.
The cause EIM is the EIM with the completed rule.
The predecessor EIM of a completed EIM
will be found in the Earley set that is origin
of the cause
and will have the LHS of the completed rule
as its postdot symbol.
\item
For an initial EIM, 
the initialization of the Earley tables
is its EIM-causality.
\end{itemize}

\alg{Leo-ordered} proceeds the same as \Leo{},
first performing its scanning operation,
then visiting every item of the Earley set
for its completion operation.
However, to guarantee that all Earley items will
appear in the Earley set in an order consistent
with their grouping into AHFA states,
a buffering operation is interposed.
An new Earley item is first inserted into a buffer.
Each Earley item to be added after that is checked
to see if it is co-occurring.
If it is co-occurring, it also is added to the buffer.
If it a new Earley item is not co-occurring, the buffer
is processed so that the new Earley item can be inserted into
an empty buffer.
The buffer is also processed and emptied at the end of
processing for each Earley set.

Processing a buffer consists of sorting its Earley items,
adding them to the current Earley set,
and emptying the buffer.
The sorting order is fixed.
Earley items that would be duplicates are never added
to an Earley set.

In effect, the buffer uses the idea of co-occurrence in
an AHFA state to ensure the eager addition of Earley items
which belong to the same AHFA state, as soon as any one
of them is added.
The normal \Leo{} is then allowed to proceed.
This may attempt to add an Earley item already added
due to co-occurrence, but that is harmless,
since duplicate Earley items are not added to an Earley set.

Note that no complexity results will be
claimed for \alg{Leo-ordered}.
While the author conjectures that \alg{Leo-ordered}
has the same bounds as \Leo{},
\alg{Leo-ordered} is a theoretical construct to show
correctness.
Its purpose is to show that \Leo{} models \Marpa{}.

When each Earley item is added to the
buffer.
\begin{itemize}
\item If the added Earley item has a nulling predot or postdot
symbol, the dot is moved over the nulling symbols,
and the resulting Earley items are also added.
\item All co-occurring Earley items are added.
\end{itemize}
As new items are added,
these two steps are performed repeatedly,
until no more new Earley items can be added.

The EIM-group buffer is processed by sorting it into
some fixed order,
then adding it to the Earley set.

We now show that \alg{Leo-ordered} produces
all the Earley items of \Leo.

\begin{theorem}
$\operatorname{table}(\alg{Leo-ordered},g)
\supset
\operatorname{table}(\alg{Leo},g)$
\end{theorem}

\begin{proof}
By the construction of the AHFA states,
we note that the buffering does all the work
of prediction and $\epsilon$-transition for
\alg{Leo-modified}.
Because of this,
the prediction step of the pseudocode
\Leo~\cite[page 179]{Leo1991}
may be omitted without effect.
For \alg{Leo-modified},
we may also ignore \Leo's scans of nullable symbols,
as they contribute nothing beyond what what is done
in the buffering.

We can show that,
with buffering as described and grammars as rewritten
for Marpa,
the reordering of the predictions in an Earley set $i$
is without effect on Earley set $i$.
In \alg{Leo-modified}.
no new Earley items can be introduced into the current
Earley set as a result of a prediction Earley item,
once that prediction has left the buffer.
This is because in the Marpa grammar, there are no
proper nullable symbols,
and all of the $\epsilon$-transitions
have already been performed in the buffer.

The pseudo-code of \Leo~\cite[page 179]{Leo1991}
specifies that all items in an Earley set must be visited
for the completion operation,
and assumes that $\epsilon$-transitions are performed,
both of which \alg{Leo-modified} clearly does.
But the pseudocode does not specify the order that
the Earley items will be visited
for the completion operation,
or the details of how
$\epsilon$-transitions are performed.
In both these respects, then
\alg{Leo-modified} is a more detailed
specification of \Leo.

From the foregoing, we see that all 
the steps of \Leo{} are either omitted
without effect,
or executed as specified in \Leo{}.
Therefore \alg{Leo-ordered} produces at least those
Earley items produced by \Leo.
\end{proof}

\section{Tagged Earley item sequence correctness}

It is necessary to show that the tagged Earley item sequences
in \alg{Leo-ordered} are complete and consistent --
that is, they they contain all, and only
the dotted rules for the appropriate AHFA state.
Because dotted rules can appear in more than one AHFA state,
the dotted rules which co-occur may not be the full
set in an AHFA-pair, 
and also can be a subset of more than one AHFA-pair.

\begin{lemma}
All the
Marpa-valid tagged Earley items 
in a tagged Earley item sequence are consistent --
they correspond
to the dotted rules of a single AHFA state pair.
\end{lemma}

\begin{proof}
As a reminder, a tagged Earley item is Marpa-valid
if it does not have a nullable postdot symbol.
Consider the set of EIMT's that result from
the $\epsilon$-transitions of a single EIMT.
(In the trivial case, where there are no nullable
symbols before or after the dot, the only member
of this set is the original EIMT.)
Only one member of this set can be a Marpa-valid EIMT.
For the purpose of this proof,
every member of this set
can be considered as the equivalent of the Marpa-valid
EIMT and duplicates can be ignored.

We now show, by induction,
that the EIMT buffer will never become inconsistent.
We take as the basis of the induction the introduction
of the first EIMT.
Trivially, a buffer with a single EIMT is consistent.
We take as the induction hypothesis that the buffer is
consistent,
and show the induction step by cases.

Marpa-valid items added to the EIMT buffer after
the first are either co-occurring EIMT's,
or are EIMT's with the same AHFA-causality.
If an EIMT is added because it
co-occurs with an EIMT previously added
to the sequence,
it belongs to the same AHFA pair as
the EIMT previously added.
Therefore, the buffer remains consistent.

The remaining case is that where
a newly added EIMT shares the same AHFA-causality as an
Earley item already added to the sequence.
The definition of AHFA-causality tracks the
definition of the AHFA,
so that Two EIMT's with the same AHFA-causality must
belong to the same AHFA pair.
The buffer will remain consistent.

These two cases show the induction step,
and complete the inductnion and the proof.
\end{proof}

\begin{lemma}
Each sequence of tagged Earley items 
is complete -- it contains
all the dotted rules of some AHFA state pair.
\end{lemma}

\begin{proof}
We will show this by induction on the Earley sets.
The induction hypothesis is that any EIMT that
has left the buffer and been added to an Earley set
is part of a complete EIMT sequence.

As the basis of the induction.
Earley set 0 contains only one EIMT sequence,
and this contains a predicted start rule,
and EIMT's for its co-occurring dotted rules.
The predicted start rule does not appear in more
than one AHFA pair, and therefore its
co-occurring dotted rules are the full 
set of rules for that AHFA pair.

The dotted rules in every AHFA pair can be seen as
effects and first causes.
If all those dotted rules which are first causes
are present, then clearly all their effects will be.
The initial AHFA was dealt with in the basis
of the induction.
For all others, the predicted rules are effects
of the discovered rules.
So to show the induction step, we must show that 
the discovered rules in an EIMT sequence will be the
full set of discovered rules for some AHFA state.

Consider the scan of a particular token.
By the definition of \alg{Leo-ordered},
the scan considers its
predecessor Earley items in an order
which ensures that, if predecessor EIMT's
are part of a single EIMT sequence
are considered in sequence.
This ensures that all cases of the same AHFA
causality are considered in sequence.

By construction of an AHFA, all discovered dotted
rules are transitions over a single symbol from
exactly one other AHFA state.
Scans are always of non-nullable symbols,
so that predecessor EIMT's are in the Earley
set prior to the current one,
and therefore by the induction hypothesis,
must be part of a complete EIMT sequence.
Therefore the scan will examine
a predecessor corresponding to every dotted rule of
the predecessor AHFA state,
and will add to the sequence in the buffer
an EIMT corresponding to every dotted rule
that results from 
the transition from the predecessor AHFA over
the scanned symbol.
We know that this new set of discovered rules
correspond to an AHFA state because,
for every possible transition from
an AHFA state,
the construction of an AHFA ensures that
a "to-state" exists.
(In effect, the scan duplicates the construction
of the AHFA.)

The proof for the completion operation
is the same as that for the scan operation.
For the completion operation, we can also
assume that the predecessor EIMT's are part
of a complete EIMT sequence,
though the argument is slightly different.
In the case of completions,
we know
that AHFA transitions are always
over non-nullable symbols,
and therefore that the predecessor EIMT's are
in a previous Earley set.
From these, and the induction hypothesis,
we can assume the predecessor EIMT sequence
is complete.

This completes the induction step, the induction
and the proof.

\end{proof}

\section{Ordered Leo Correctness}

\begin{theorem}
$\myL{\alg{Marpa},g} = \myL{g}$
\end{theorem}

\section{Marpa recognizer time complexity}
\label{s:marpa:complexity}

\begin{theorem}\label{t:rs:constant-per-EIM}
All time in \Rubyslippers{} can be allocated
to the Earley items,
and in such a way that processing each Earley item
requires $\order{1}$ time.
\end{theorem}

\begin{proof}
It is known that the time taken by
the traditional Earley recognizer
for its operations
operations can be done in amortized $\order(1)$
time per Earley item~\cite[Vol. 1, pages 326-327]{AU1972}.
Inspection of the algorithm for \Rubyslippers{} will show
that any operations considered to be new to it
can be assigned to the Earley items in
an obivous way,
and 
trivially shown to be $\order{1}$.
\end{proof}

\begin{theorem}
For all context-free grammars,
the time and space complexity
of \Rubyslippers{} is as good or better
than that of Earley's algorithm.
\end{theorem}

\begin{proof}
The time and space complexity
results for $\Earley$ are based
on establishing the order of the number of Earley items,
and showing that $\Earley$ takes
$\order{1}$ time and 
$\order{1}$ space
for each Earley
item.~\cite[Vol. 1, pages 325-327]{AU1972}.
By Theorem \ref{a:rs:eim-order},
for any grammar, the order of the number of Earley
items is the same in \Rubyslippers{} as it is
in $\Earley$.
It is trivial to show that the space requirements for
\Rubyslippers{} are $\order{1}$ time for each Earley item.
By Theorem ref{t:rs:constant-per-EIM},
the time requirements for
\Rubyslippers{} are also $\order{1}$ for each Earley item.
Therefore the time and space complexity results for $\Earley$
hold for \Rubyslippers.
\end{proof}

\begin{theorem}
For all context-free grammars,
the time and space complexity
of \Marpa{} is as good or better
than that of Earley's algorithm.
\end{theorem}

\begin{theorem}
For every LR-regular grammar,
\Marpa{} runs in linear time and linear space.
\end{theorem}

\section{Generalizing the grammar}
\label{s:generalizing}

This section deals with certain modifications
to the
definition of a grammar made by Marpa.
They
are minor from a theoretical point of view,
and their discussion has been deferred so that the
proofs up to this point might more closely follow
tradition.

First, Marpa does not require that $\symset{lh}$
and $\symset{term}$ be disjoint.
In other words a Marpa symbol may be both a terminal
and a LHS.
This expansion of the grammar definition 
is made without loss of generalization,
or effect on the results so far.

Second, Marpa's input model is a generalization of
the traditional input stream model
used so far.
Marpa's input is a set of tokens,
$toks$,
whose elements are triples of symbol,
start location and end location:
$(\sym{t}, \loc{start}, length)$
such that $\sym{t} \in \symset{term} \wedge length \ge 1 \wedge \loc{start} \ge 0$.
The size of the input, $\size{toks}$ is the maximum over
$toks$ of $\loc{start}+length$.
Tokens may overlap,
but gaps are not allowed:
$\forall \loc{i} (\exists \token{t} \sep \token{t} \in toks
\wedge t = (\sym{t}, \loc{start}, length)
\wedge \loc{start} \le \loc{i} \le \loc{start}+length$.

The traditional input stream is the special case of
a Marpa input stream where 
$\forall \token{tok} \sep \token{tok} \in toks \implies 
\token{tok} = (\sym{s}, \loc{start}, 1)
$
and
$\forall \token{tok1}, \token{tok2} \in toks \sep
\token{tok1} = (\sym{s1}, \loc{start1}, 1)
\wedge
\token{tok2} = (\sym{s2}, \loc{start2}, 1)
\wedge (\loc{start1} = \loc{start2} \implies \token{tok1} = \token{tok2})
$.

The correctness results hold for Marpa input streams,
but to preserve the time complexity bounds,
certain restriction must be imposed on them.
Call an Earley set, $\es{table[j]}$
a "lookahead set" at $\loc{i}$
if $j>i \wedge$
and there exists some token $\token{tok} = (\sym{s}, \loc{start}, length)$
such that $\loc{start} \le i \wedge \loc{start}+length = \loc{j}$.
If, at every location,
the number of tokens which start there,
and the number of lookahead sets in play at that location
is less than a finite constant,
then Earley items can be added in amortized constant time
and the complexity results for
\Rubyslippers{} and \Marpa{}
stand.

\bibliographystyle{plain}

\begin{thebibliography}{10}

\bibitem{AU1972}
Alfred H.~Aho and Jeffrey D.~Ullman.
\newblock The Theory of Parsing, Translation, and Computing
\newblock Prentice-Hall, Englewood Cliff, N.J., 1972.

\bibitem{AH2002}
John~Aycock and R.~Nigel~Horspool.
\newblock Practical Earley Parsing
\newblock {\em The Computer Journal},
    Vol. 45, No. 6, 2002, pp. 620-630.

\bibitem{Earley1970}
J.~Earley.
\newblock An efficient context-free parsing algorithm.
\newblock {\em Communications of the Association for Computing Machinery},
  13(2):94--102, 1970.

\bibitem{GJ2008}
Dirk~Grune and Ceriel~J.H Jacobs
\newblock {\em Parsing Techniques: A Practical Guide}.
\newblock Springer, Amsterdam, 2008.

\bibitem{Marpa-XS}
Jeffrey~Kegler, 2011: Marpa-XS-1.002000.
\newblock [ Available online at http://search.cpan.org/dist/Marpa-XS/. ]

\bibitem{Leo1991}
J.~M. I.~M. Leo.
\newblock A general context-free parsing algorithm running in linear time on
  every {LR($k$)} grammar without using lookahead.
\newblock {\em Theoretical Computer Science}, 82:165--176, 1991.

\end{thebibliography}
 
\end{document}
