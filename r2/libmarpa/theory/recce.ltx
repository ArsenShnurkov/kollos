% Copyright 2012 Jeffrey Kegler
% This document is licensed under
% a Creative Commons Attribution-NoDerivs 3.0 United States License.
\documentclass[12pt]{amsart}
\usepackage{algorithm}
\usepackage{algpseudocode}

\newcommand{\comment}[1]{}
\newcommand{\myspace}{\mbox{ }\ \ \ \ }
\newcommand{\myspacem}{\;\;\;\;\;}
\newcommand{\sep}{\,\mid\,}
\newcommand{\mydot}{\raisebox{.15em}{\tiny $\,\bullet\,$}}
\newcommand{\size}[1]{\left | {#1} \right |} 
\newcommand{\order}[1]{{\mathcal O}(#1)}

\newcommand{\var}[1]{\ensuremath{\textbf{#1}}}

\newcommand{\cfg}{CFG}
\newcommand{\nonterm}{\mbox{$V_{{\rm N}}$}}
\newcommand{\myterm}{\mbox{$V_{{\rm T}}$}}

\newcommand{\de}{\rightarrow}
\newcommand{\derivg}[1]{\mathrel{\mbox{$\:\Rightarrow\:$}}}
\newcommand{\derivrg}[2]{\mathrel{\mbox{$\:\stackrel{\!{#1}}%
        {\Rightarrow\!}\:$}}}

\newcommand{\ep}{\varepsilon}

\newcommand{\ah}[1]{#1_{AH}}
\newcommand{\aim}[1]{#1_{AIM}}
\newcommand{\eim}[1]{#1_{EIM}}
\newcommand{\es}[1]{#1_{ES}}
\newcommand{\loc}[1]{#1_{LOC}}
\newcommand{\production}[1]{#1_{RULE}}
\newcommand{\sym}[1]{#1_{SYM}}
\newcommand{\symset}[1]{#1_{\lbrace SYM \rbrace} }
\newcommand{\term}[1]{#1_{TERM}}
\newcommand{\token}[1]{#1_{TOK}}
\newcommand{\alg}[1]{\textsc{#1}}
\newcommand{\AH}{\ensuremath{\alg{AH}}}
\newcommand{\Earley}{\ensuremath{\alg{Earley}}}
\newcommand{\Leo}{\ensuremath{\alg{Leo}}}
\newcommand{\Marpa}{\ensuremath{\alg{Marpa}}}
\newcommand{\Rubyslippers}{\ensuremath{\alg{Ruby-Slippers}}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\DeclareMathOperator{\scanned}{scanned}
\DeclareMathOperator{\Hyp}{Hyp}
\newcommand\myL[1]{\operatorname{L}(#1)}
\newcommand\table[1]{\es{\operatorname{table}(#1)}}

% I want to use 'call' outside of pseudocode
\newcommand\call[2]{\textproc{#1}\ifthenelse{\equal{#2}{}}{}{(#2)}}%

\hyphenation{ALGOL}

\begin{document}

\title{The Marpa Recognizer}

\author{Jeffrey Kegler}
\thanks{
Copyright \copyright\ 2012 Jeffrey Kegler.
This document is licensed under
a Creative Commons Attribution-NoDerivs 3.0 United States License.
}

\begin{abstract}
This paper reports describes the recognizer portion
of the Marpa algorithm.
The Marpa algorithm is a practical, and fully implemented,
algorithm for the recognition,
parsing and evaluation of context-free grammars.
Marpa's recognizer is based
an Earley's algorithm,
and merges the improvements to Earley's
from Leo~\cite{Leo1991}
and Aycock and Horspool~\cite{AH2002}.
Their combination in the marpa parse engine
has an added feature:
It makes available,
before each token is scanned,
full knowledge of the state of the parse so far.
This allows input to be altered as the parse progresses,
which can be an extremely powerful technique.
\end{abstract}

\maketitle

\section{THIS IS AN EARLY DRAFT}

This a very early draft of this paper,
and is largely unchecked.
The spirit and culture of the open source community
currently dictates that works of this kind and at
this very early stage be made
available on-line in public repositories.
The reader should see the availability of this document
as compliance with that spirit and culture,
and not as in any way suggesting or
encouraging reliance on its contents.

\section{Introduction}

Despite the promise of general context-free parsing,
and the strong academic literature behind it,
it has never been incorporated into a tool
as widely available as yacc or
regular expressions.
The Marpa project was begun to end this neglect by
pulling the best results from the literature,
and to turn them into a widely-available tool.
A stable version of this tool, Marpa::XS~\cite{Marpa-XS},
was uploaded to the CPAN Perl archive
on Solstice Day in 2011.

Marpa::XS is a complete implementation of a parser
generator.
Its recognizer is built around a parse engine,
which is new, but which also owes a great debt
to previous work.

\section{Preliminaries}
\label{s:prel}

I assume familiarity with standard grammar notation
(pages 14-15 in Aho and Ullman~\cite{AU1972}).
In the past,
The type system required to support
a theory of parsing has been taken
as a challenge to the typographic
imagination,
and often resulted in one to the eyesight.
This paper will often
supplement the standard notation,
using subscripts to indicate the commonly occuring types.
In general, variable will contain a capital
letter, constants will be all lower case.
For example, $\sym{a}$ and $\sym{b}$ would be symbol constants,
while $\sym{X}$ will be a variable whose value is a symbol.

Where $ABC$ is a set of symbols,
let $ABC^\ast$ be the set of all strings formed
from those symbols.
Let $ABC^+$ be the subset of $ABC^\ast$ that
contains all of its elements that are not of zero length.

For the purposes of this paper consider,
without loss of generality,
a grammar $g$,
and a set of symbols, $alphabet$.
Call the language of $g$, $\myL{g}$,
where $\myL{g} \in alphabet^\ast$
Let its input be $w$, $w \in alphabet$.
Divide $alphabet$ into two disjoint sets,
$term$ and $nonterm$.

For the rewriting, designate a set of duples, $rules$,
where $\forall Rule \sep Rule \in rules$,
$Rule$ takes the form $\sym{l} \de r$,
where $\sym{l} \in nonterm$ and 
$r \in (nonterm \cup term)^+$. 
$sym{l}$ is referred to as the left hand side (LHS)
of $Rule$.
$r$ is referred to as the right hand side (RHS)
of $Rule$.
This definition differs from the traditional one in
that the empty RHS is not allowed.
Let one symbol $\sym{start}$, $\sym{start} \in nonterm$,
be a dedicated start symbol.

The grammar $g$ can be defined as the 4-tuple
$(term, nonterm, rules, \sym{start})$.
Without loss of generality,
it is assumed that $g$ is "augmented",
so that there is a rule $\sym{start} \de \sym{old-start}$,
and that $\sym{start}$ is not the LHS of any other rule,
or in the RHS of any rule.

I was already noted
that no rules of $g$ is allowed to be empty -- to
have a zero-length RHS.
Further, no symbol may be a proper nullable --
all symbol must be either nulling or non-nullable.
These restrictions follow Aycock and Horspool~\cite{AH2002}.

Aycock and Horspool did allow a single empty start rule
to deal with null parses.
Marpa eliminates all need for empty rules in its grammar
by dealing with null parses as a special case.
Marpa also deals with
trivial grammars (those which recognize only the null string)
as a special case.

The elimination of empty rules and proper nullables
is done by rewriting the grammar.
This can be done without loss of generality
and, In their paper~\cite{AH2002},
Aycock and Horspool
show how to do this
without effect on the time complexity
as a function of the input size.
Very importantly,
this rewrite is done in such a way that the semantics
of the original grammar can be efficiently reconstructed
at evaluation time.

In this paper, $Earley$ will refer to the Earley's original
recognizer~\cite{Earley1970}.
$Leo$ will refer to Leo's revision of $Earley$
as described in his 1991 paper~\cite{Leo1991}.
$AH$ will refer to the Aycock and Horsool's revision
of $Earley$
as described in their 2002 paper~\cite{AH2002}.
Where $\alg{Recce}$ is a recognizer,
$\myL{\alg{Recce},g}$ will be the language accepted by $\alg{Recce}$
when parsing grammar $g$.

\section{AHFA States}
\label{s:AHFA}

In this paper a
"split LR(0) $\epsilon$-DFA"
a described by Aycock and Horspool~\cite{AH2002},
will be called an Aycock-Horspool Finite Automaton,
or AHFA.
Let FA be the AHFA derived from $G$.
as described in~\cite{AH2002}.

A full description of how to derive an AHFA in theory
can be found in \cite{AH2002},
and examples of how to derive it in practice
can be found in the code for various release of Marpa.
Here I will just summarize the central ideas behind AHFA's.

Aycock and Horspool based their AHFA's
on a few observations.
First, in practice, Earley items with the same dotted rule
often appear in groups in the Earley sets,
where the entire shares the same origin.
Second, there was already in the literature a method
for associating groups of dotted rules that often appear together
when parsing.
This method was the LR(0) DFA used in the much-studied
LALR and LR parsers.
Third, the LR(0) items that are the components of LR(0)
states are, exactly, dotted rules.
Fourth, by taking into account symbols which derive the
null string, the LR(0) DFA could be turned into an
LR(0) $\epsilon$-DFA, which would be even more effective
at grouping dotted rules which occur together.

AHFA states are sets of dotted rules.
Aycock and Horspool realized that by changing Earley items
to track AHFA states, instead of individual dotted rules,
the size of Earley sets could be reduced,
and Earley's algorithm made faster in practice.
In short, then, AHFA states are a shorthand that Earley items
can use for groups of dotted rules that occur together frequently.
The original Earley items could be represented as $(r_{DOTTED}, origin)$
duples, where $r_{DOTTED}$ is a dotted rule.
Aycock and Horspool modified their Earley items to be $(\ah{L}, origin)$
duples, where $\ah{L}$ is an AHFA state.

\section{The Ruby Slippers Recognizer}
\label{s:recce}

\begin{algorithm}[H]
\caption{Ruby Slippers Initialization}\label{a:rs:initial}
\begin{algorithmic}[1] 
\Procedure{Initial}{$i,a$}
\State \Call{AddItems}{$0, \ah{start}, 0$}
\EndProcedure
\end{algorithmic} 
\end{algorithm}

\begin{algorithm}[H]
\caption{Ruby Slippers Scanning}\label{a:rs:scan}
\begin{algorithmic}[1] 
\Procedure{Scan}{$i,a$}
\Comment{Scan a token $a$, which starts at Earley set $i$}
\For{each Earley item $\eim{x}$ in $ES[i]$}
\State $\ah{to} \gets GOTO(\operatorname{AHFA-of}(\eim{x}), a)$
\State \Call{Add EIM Pair}{$i+1, \ah{to}, \operatorname{Origin-of}(\eim{x}$}
\EndFor
\EndProcedure
\end{algorithmic} 
\end{algorithm}

\begin{algorithm}[H]
\caption{Ruby Slippers Completion}\label{a:rs:complete}
\begin{algorithmic}[1] 
\Procedure{Complete}{$i_{LOC}$}
\For{each Earley item $\eim{work} \in ES[i]$}
\For{each completed rule, $\production{C}$}
\For{each $\eim{predecessor} \in ES[\operatorname{Origin-of} \eim{work}]$}
\State $\ah{to} \gets GOTO(\operatorname{AHFA-of} \eim{predecessor}, LHS_{SYM})$
\State \Call{Add EIM Pair}{$to_{AHFA}, \operatorname{Origin-of} \eim{predecessor}$}
\EndFor
\EndFor
\EndFor
\State Construct $transitions[i]$
\EndProcedure
\end{algorithmic} 
\end{algorithm}

\begin{algorithm}[H]
\caption{Ruby Slippers, Adding EIM Pairs}\label{a:rs:pair}
\begin{algorithmic}[1] 
\Function{Add EIM Pair}{$i_{LOC},\ah{discovered},origin_{LOC}$}
\State $\ah{predicted} \gets \Lambda$
\State $\eim{discovered} \gets \Lambda$
\If{$\ah{to} \neq \Lambda$}
\State $\eim{discovered} \gets [\ah{to}, origin_{LOC}]$
\If{$\eim{discovered}$ is new}
\State Add $\eim{discovered}$ to $ES[i]$
\State $\ah{predicted} \gets GOTO(\ah{to}, \epsilon)$
\EndIf
\EndIf
\If{$predicted_{AHFA} \neq \Lambda$}
\State $\eim{predicted} \gets [\ah{predicted}, i]$
\If{$eim{predicted}$ is new}
\State Add $\eim{predicted}$ to $ES[i]$
\EndIf
\EndIf
\State \textbf{Return} $\eim{discovered}$
\EndFunction
\end{algorithmic} 
\end{algorithm} 

\section{Ruby Slippers Correctness}
\label{s:leo:proof}

In this section I will show that a
Ruby Slippers
parse engine without Leo items
produces the same Earley items
as AH2002,
so that
correctness follows
from the correctness proofs
in AH2002\cite{AH2002}.
In the following,
$\es{AH[i]}$ is Earley set $i$ according
to AH2002;
$\es{RS[i]}$ is Earley set $i$ according
to the Ruby Slippers algorithm;
and $\scanned(\es{x})$ is the subset
of $\es{x}$,
that consists of all, and of only,
the its scanned Earley items.
Let the induction hypothesis, $Hyp(n)$ be
$\forall i, 0<i<n, \es{AH[i]} = \es{RS[i]}$.

\begin{lemma}\label{t:rs:correctness:basis}
$Hyp(0)$
\end{lemma}
\begin{proof}
Note that Earley set 0 never contains scanned items.
By inspection of \ref{a:rs:initial},
and comparision with AH2002, we have
$\es{AH[0]} = \es{RS[0]}$.
\end{proof}

\begin{lemma}\label{t:rs:correctness:step1}
In \Rubyslippers,
Let $t-before$ be the point in time just before a call 
to
$\call{Scan}{i,\term{input[i]}}$,
and $t-after$ the point just after.
If $\Hyp(i)$ at $t-before$,
then at $t-after$
$$ \Hyp(i) \wedge \table{\AH, i+1} = \table{\Rubyslippers, i+1} $$
\end{lemma}
\begin{proof}
From inspection of \ref{a:rs:scan}
and comparision with AH2002.
\end{proof}

\begin{lemma}\label{t:rs:correctness:step2}
\begin{multline*}
\Hyp(i) \wedge \scanned(\es{AH[i+1]}) = \scanned(\es{RS[i+1]})
\wedge \call{Complete}{i+1} \Longrightarrow \\
\Hyp(i+1)
\end{multline*}
\end{lemma}
\begin{proof}
From inspection of \ref{a:rs:complete}
and comparision with AH2002.
\end{proof}

\begin{theorem}\label{t:rs:ah-reduction}
$\forall n( 0<n<\size{input} \implies \Hyp(n))$
\end{theorem}

\begin{proof}
By induction, with \ref{t:rs:correctness:basis}
as the basis and 
\ref{t:rs:correctness:step1}
and 
\ref{t:rs:correctness:step2}
as the steps.
\end{proof}

\begin{theorem}\label{t:ah:correct}
$\myL{\AH ,g} = \myL{g}$
\end{theorem}

\begin{proof}
This is asserted in~\cite{AH2002},
and follows
from the formally stated theorems
and the detailed construction of the
AHFA presented in that paper.
\end{proof}

\begin{theorem}
$\myL{\Rubyslippers ,g} = \myL{g}$
\end{theorem}

\begin{proof}
From Theorem
\ref{t:ah:correct}
and Theorem
\ref{t:rs:ah-reduction}.
\end{proof}

\section{Ruby Slippers time complexity}
\label{s:rs:complexity}

\begin{theorem}
For all $\loc{i}$,
$\size{\table{\Rubyslippers,i}} = \size{\table{\AH,i}}$
\end{theorem}

\begin{proof}
From Theorem \ref{t:rs:ah-reduction}.
\end{proof}

On \cite[page 626]{AH2002} it is stated that
"[i]n the worst case,
each split LR(0) $\epsilon$-DFA
state
would contain a single item,
effectively reducing it to Earley's
original algorithm".
(In this paper,
n split LR(0) $\epsilon$-DFA is called an AHFA.)
No argument is presented for this statement.
In fact, an AHFA is not a partition of the dotted
rules, and
a single dotted rule can occur
in more than one AHFA state.
This does not happen frequently,
but it does happens often enough,
even in practical grammars,
that the Marpa implementation has to provide for it.

What does not seem to happen in practical grammars
the size of \AH Earley set to grow larger
than that of one of Earley's original sets.
That is, it seems the AHFA is always a win,
at least for practical grammars.

What can be shown is that the time and space
complexity of \AH is never worse than that of $\Earley$.
The next lemma is sufficient for that purpose.

\begin{lemma}
For all $\loc{i}$,
$\order{\size{\table{\AH,i}}} = \order{\size{\table{\Earley,i}}}$
\end{lemma}

A dotted rule with the dot before the first
non-nullable symbol is called a prediction.
A dotted rule with the dot after the last symbol of
a rule is called a prediction.
In the proof,
a dotted rule that is not a prediction will
be called a kernel rule.
In \AH, \Rubyslippers{} and \Marpa,
there are no dotted rules with the dot before
a nulling symbol.
Informally, nulling symbols can said to be "skipped".

Progress through a rule can be thought of as moving the
dot forward over one of the rule's symbols.
Consider a dotted rule $\aim{\var{d-to}}$
results from moving the dot forward,
skipping nulling symbols,
from another dotted rule
$\aim{\var{d-from}}$.
$\aim{\var{d-from}}$ is called the predecessor
of $\aim{\var{d-to}}$; and
$\aim{\var{d-to}}$ is called the successor
of $\aim{\var{d-from}}$.

\begin{proof}
AHFA states are of two types: predicted and discovered.
Predicted AHFA state are
those which contain predictions (the initial states and the non-kernel states).
Discovered AHFA state are
and those which do not (non-initial kernel states).
The predicted AHFA states are constructed from sets of symbols
by a transitive closure operation,
and therefore there cannot be more than $2^c$ predicted AHFA states,
where $c$ is the number of LHS symbols.
Since only a predicted AHFA state can contain a prediction,
no prediction occurs in more than $2^c$ AHFA states.

Kernel rules are subject to the same bound:
no kernel rule occurs in more than $2^c$ AHFA states.
This can be shown from the construction of the AHFA states.
Each occurence of a kernel rule in an AFHA state is
is the product of a transition from another
AHFA state over a symbol.
Each such transition puts the new kernel rule into
at most one new AHFA state.
There for, for each predecessor dotted rule,
there can be at most one successor.

Each traditional Earley item has a single dotted rule,
so that a translation from a traditional Earley items
can result in at most $2^c$
\AH Earley items.
So, we have
\begin{align}
\order{\size{\table{\AH,i}}} & \\
 & = 2^c \order{\size{\table{\Earley,i}}} \\
 & = \order{\size{\table{\Earley,i}}}
\end{align}

\end{proof}

\begin{theorem}\label{t:rs:constant-per-EIM}
All time in \Rubyslippers{} can be allocated
to the Earley items,
and in such a way that processing each Earley item
requires $\order{1}$ time.
\end{theorem}

\begin{proof}
It is known that the time taken by
the traditional Earley recognizer
for its operations
operations can be done in amortized $\order(1)$
time per Earley item~\cite[Vol. 1, pages 326-327]{AU1972}.
Inspection of the algorithm for \Rubyslippers{} will show
that any operations considered to be new to it
can be assigned to the Earley items in
an obivous way,
and 
trivially shown to be $\order{1}$.
\end{proof}

\begin{theorem}
For all context-free grammars,
the time and space complexity
of \Rubyslippers{} is as good or better
than that of Earley's algorithm.
\end{theorem}

\begin{proof}
The time and space complexity
results for $\Earley$ are based
on establishing the order of the number of Earley items,
and showing that $\Earley$ takes
$\order{1}$ time and 
$\order{1}$ space
for each Earley
item.~\cite[Vol. 1, pages 325-327]{AU1972}.
By Theorem \ref{a:rs:eim-order},
for any grammar, the order of the number of Earley
items is the same in \Rubyslippers{} as it is
in $\Earley$.
It is trivial to show that the space requirements for
\Rubyslippers{} are $\order{1}$ time for each Earley item.
By Theorem ref{t:rs:constant-per-EIM},
the time requirements for
\Rubyslippers{} are also $\order{1}$ for each Earley item.
Therefore the time and space complexity results for $\Earley$
hold for \Rubyslippers.
\end{proof}

\section{The Marpa recognizer}
\label{s:rs}

\section{The "as-if" ordering}
\label{s:rs}

The correctness proof of \Marpa{}
will proceed by
induction on an "as-if" reordering of
the operations in both \Leo{}
and \Marpa{}.
This ordering is not the actual one
used in either algorithm,
but it can be shown that,
once parsing is complete,
the ordering is that same "as if"
the actual \Marpa{} and \Leo{} 
ordering were used.

As mentioned, Marpa grammars are 
rewritten to eliminate proper nullable
symbols and empty rules.
adn the AHFA states collect dotted rules
which always appear together.
Some AHFA states ("kernel states")
have an $\epsilon$ transition
to one other AHFA state (a "non-kernel" state).
A non-kernel state never has an $\epsilon$ transition,
so and the $\epsilon$ transitions can be seen
as defined a partition of AHFA states into
sets of cardinality 1 or 2.

This following restrictions on the order
of operations in \Marpa{} exist:

\begin{itemize}
\item The initial EIM pair  must be created first.
\item For every location $i$,
all scanned EIM pairs must be added before any
completed EIM pairs.
\item For every location $i$,
all completed EIM pairs must be added before
the Leo items at $i$ can be determined.
\item For every location $i$,
All EIM pairs must be added at location $i$,
before any scanned items at locations greater
than $i$ are added.
\end{itemize}

\section{Marpa recognizer correctness}
\label{s:marpa:correctness}

\subsection{The Earley operations produce the Earley items}

\Marpa's operations may be divided into
two types: those it inherits from the traditional \Earley,
and those it inherits from Leo.
This section shows that the operations that \Marpa{}
inherits from Earley's algorithm produce AH Earley items
which translate to the same Earley items as those of \Earley.
This result will be used in a proof by induction that
the combined Leo and traditional Earley operations of
\Marpa{} are correct,
so care is taken about the order in which the results
enter the Earley sets.

We define a new algorithm,
\alg{Leo-ordered} based on \Leo.
Call an EIM-group an set of traditional Earley items which
correspond to a single AH Earley item.
We modify it so that the Earley sets are ordered,
so that all the traditional Earley items
from a single EIM-group occur together in the sequence.
Where a traditional Earley item correspond to more
than one AH Earley item in the same Earley set,
it will occur with the first EIM-group that applies.

For the purpose of this section,
two traditional Earley items are said to {bf co-occur}
if whenever either appears in an AHFA state,
the other also does.

Informally, an EIM-causality is the set of factors
which are necessary to cause the addition of a traditional
Earley item to an Earley set.
The initialization of the Earley table is one such factor.
The other factors are tokens and other Earley items.

\begin{itemize}
\item The EIM-causality of a predicted EIM is a single EIM:
the one which predicts it
by having the predicted EIM's LHS
as a postdot symbol.
\item
The EIM-causality of
a scanned EIM is its token and its predecessor EIM.
The predecessor EIM is the EIM in the previous Earley
set which had the token as its postdot symbol.
\item
The EIM-causality of a completion EIM is a pair of EIM's:
a cause EIM and a predecessor EIM.
Recall that a completion EIM is added as a result of finding
a completed rule in another EIM.
The cause EIM is the EIM with the completed rule.
The predecessor EIM of a completed EIM
will be found in the Earley set that is origin
of the cause
and will have the LHS of the completed rule
as its postdot symbol.
\item
For an initial EIM, 
the initialization of the Earley tables
is its EIM-causality.
\end{itemize}

\alg{Leo-ordered} proceeds the same as \Leo{},
first performing its scanning operation,
then visiting every item of the Earley set
for its completion operation.
However, to guarantee that all Earley items will
appear in the Earley set in an order consistent
with their grouping into AHFA states,
a buffering operation is interposed.
An new Earley item is first inserted into a buffer.
Each Earley item to be added after that is checked
to see if it is co-occuring.
If it is co-occuring, it also is added to the buffer.
If it a new Earley item is not co-occuring, the buffer
is processed so that the new Earley item can be inserted into
an empty buffer.
The buffer is also processed and emptied at the end of
processing for each Earley set.

Processing a buffer consists of sorting its Earley items,
adding them to the current Earley set,
and emptying the buffer.
The sorting order is fixed.
Earley items that would be duplicates are never added
to an Earley set.

In effect, the buffer uses the idea of co-occurrence in
an AHFA state to ensure the eager addition of Earley items
which belong to the same AHFA state, as soon as any one
of them is added.
The normal \Leo{} is then allowed to proceed.
This may attempt to add an Earley item already added
due to co-occurrence, but that is harmless,
since duplicate Earley items are not added to an Earley set.

Note that no complexity results will be
claimed for \alg{Leo-ordered}.
While the author conjectures that \alg{Leo-ordered}
has the same bounds as \Leo{},
\alg{Leo-ordered} is a theoretical construct to show
correctness.
Its purpose is to show that \Leo{} models \Marpa{}.

When each Earley item is added to the
buffer.
\begin{itemize}
\item If the added Earley item has a nulling predot or postdot
symbol, the dot is moved over the nulling symbols,
and the resulting Earley items are also added.
\item All co-occuring Earley items are added.
\end{itemize}
As new items are added,
these two steps are performed repeatedly,
until no more new Earley items can be added.

The EIM-group buffer is processed by sorting it into
some fixed order,
then adding it to the Earley set.

We now show that \alg{Leo-ordered} produces
all the Earley items of \Leo.

\begin{theorem}
\operatorname{table}(\alg{Leo-ordered},g)
\supset
\operatorname{table}(\alg{Leo},g)
\end{theorem}

\begin{proof}
By the construction of the AHFA states,
we note that the buffering does all the work
of prediction and $\epsilon$-transition for
\alg{Leo-modified}.
Because of this,
the prediction step of the pseudocode
\Leo~\cite[page 179]{Leo1991}
may be omitted without effect.
For \alg{Leo-modified},
we may also ignore \Leo's scans of nullable symbols,
as they contribute nothing beyond what what is done
in the buffering.

We can show that,
with buffering as described and grammars as rewritten
for Marpa,
the reordering of the predictions in an Earley set $i$
is without effect on Earley set $i$.
In \alg{Leo-modified}.
no new Earley items can be introduced into the current
Earley set as a result of a prediction Earley item,
once that prediction has left the buffer.
This is because in the Marpa grammar, there are no
proper nullable symbols,
and all of the $\epsilon$-transitions
have already been performed in the buffer.

The pseudo-code of \Leo~\cite[page 179]{Leo1991}
specifies that all items in an Earley set must be visited
for the completion operation,
and assumes that $\epsilon$-transitions are performed,
both of which \alg{Leo-modified} clearly does.
But the pseudocode does not specify the order that
the Earley items will be visited
for the completion operation,
or the details of how
$\epsilon$-transitions are performed.
In both these respects, then
\alg{Leo-modified} is a more detailed
specification of \Leo.

From the foregoing, we see that all 
the steps of \Leo{} are either omitted
without effect,
or executed as specified in \Leo{}.
Therefore \alg{Leo-modified} produces at least those
Earley items produced by \Leo.
\end{proof}

\begin{theorem}
$\myL{\alg{Marpa},g} = \myL{g}$
\end{theorem}

\section{Marpa recognizer time complexity}
\label{s:marpa:complexity}

\begin{theorem}
For all context-free grammars,
the time and space complexity
of \Marpa{} is as good or better
than that of Earley's algorithm.
\end{theorem}

\begin{theorem}
For every LR-regular grammar,
\Marpa{} runs in linear time and linear space.
\end{theorem}

\section{Generalizing the grammar}
\label{s:generalizing}

This section deals with certain modifications
to the
definition of a grammar made by Marpa.
They
are minor from a theoretical point of view,
and their discussion has been deferred so that the
proofs up to this point might more closely follow
tradition.

First, Marpa does not require that $\symset{lh}$
and $\symset{term}$ be disjoint.
In other words a Marpa symbol may be both a terminal
and a LHS.
This expansion of the grammar definition 
is made without loss of generalization,
or effect on the results so far.

Second, Marpa's input model is a generalization of
the traditional input stream model
used so far.
Marpa's input is a set of tokens,
$toks$,
whose elements are triples of symbol,
start location and end location:
$(\sym{t}, \loc{start}, length)$
such that $\sym{t} \in \symset{term} \wedge length \ge 1 \wedge \loc{start} \ge 0$.
The size of the input, $\size{toks}$ is the maximum over
$toks$ of $\loc{start}+length$.
Tokens may overlap,
but gaps are not allowed:
$\forall \loc{i} (\exists \token{t} \sep \token{t} \in toks
\wedge t = (\sym{t}, \loc{start}, length)
\wedge \loc{start} \le \loc{i} \le \loc{start}+length$.

The traditional input stream is the special case of
a Marpa input stream where 
$\forall \token{tok} \sep \token{tok} \in toks \implies 
\token{tok} = (\sym{s}, \loc{start}, 1)
$
and
$\forall \token{tok1}, \token{tok2} \in toks \sep
\token{tok1} = (\sym{s1}, \loc{start1}, 1)
\wedge
\token{tok2} = (\sym{s2}, \loc{start2}, 1)
\wedge (\loc{start1} = \loc{start2} \implies \token{tok1} = \token{tok2})
$.

The correctness results hold for Marpa input streams,
but to preserve the time complexity bounds,
certain restriction must be imposed on them.
Call an Earley set, $\es{table[j]}$
a "lookahead set" at $\loc{i}$
if $j>i \wedge$
and there exists some token $\token{tok} = (\sym{s}, \loc{start}, length)$
such that $\loc{start} \le i \wedge \loc{start}+length = \loc{j}$.
If, at every location,
the number of tokens which start there,
and the number of lookahead sets in play at that location
is less than a finite constant,
then Earley items can be added in amortized constant time
and the complexity results for
\Rubyslippers{} and \Marpa{}
stand.

\bibliographystyle{plain}

\begin{thebibliography}{10}

\bibitem{AU1972}
Alfred H.~Aho and Jeffrey D.~Ullman.
\newblock The Theory of Parsing, Translation, and Computing
\newblock Prentice-Hall, Englewood Cliff, N.J., 1972.

\bibitem{AH2002}
John~Aycock and R.~Nigel~Horspool.
\newblock Practical Earley Parsing
\newblock {\em The Computer Journal},
    Vol. 45, No. 6, 2002, pp. 620-630.

\bibitem{Earley1970}
J.~Earley.
\newblock An efficient context-free parsing algorithm.
\newblock {\em Communications of the Association for Computing Machinery},
  13(2):94--102, 1970.

\bibitem{GJ2008}
Dirk~Grune and Ceriel~J.H Jacobs
\newblock {\em Parsing Techniques: A Practical Guide}.
\newblock Springer, Amsterdam, 2008.

\bibitem{Marpa-XS}
Jeffrey~Kegler, 2011: Marpa-XS-1.002000.
\newblock [ Available online at http://search.cpan.org/dist/Marpa-XS/. ]

\bibitem{Leo1991}
J.~M. I.~M. Leo.
\newblock A general context-free parsing algorithm running in linear time on
  every {LR($k$)} grammar without using lookahead.
\newblock {\em Theoretical Computer Science}, 82:165--176, 1991.

\end{thebibliography}
 
\end{document}
